{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrpIdT95CTcH",
        "outputId": "eca059c3-8256-4a97-f614-83c102817e08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OmYOxnziCNR-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
        "from torchvision import models, transforms\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "pd.set_option('display.max_columns', 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **미션 3**"
      ],
      "metadata": {
        "id": "AYuQDR2lyMK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **미션 3-1**\n",
        "- 2-1에서 구한 유효한 라벨링 데이터만 따로 분리하여 100명 응답자의 “스타일 선호 정보표”를 구한다."
      ],
      "metadata": {
        "id": "jaHA08QUasZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "협업 필터링이란 기존에 가지고 있는 데이터를 평가하여 새로운 데이터에 대해 유사한 user, item을 기반으로 추천을 해주는 방법이다. 이때 사용자 기반 협업 필터링은 새로운 평가를 내리려는 사용자와 유사한 사용자의 아이템 평가를 통해서 추천하는 방법이다. 사용자-아이템 상호작용 행렬이 있다고 할 때 row를 기준으로 평가를 진행한다. 아이템 기반 협업 필터링은 새로운 평가를 내리려는 아이템과 유사한 아이템의 사용자 평가를 통해서 추천하는 방법이다."
      ],
      "metadata": {
        "id": "BiZEJbX7awIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1) 스타일 선호 정보표를 이용한 유틸리티 행렬 계산**"
      ],
      "metadata": {
        "id": "SF_KwHSLa16g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ss9-ZSPpafP1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6d42e22f-7b21-4222-d458-7910ee503dfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                   스타일 선호_x                  스타일 비선호_x  \\\n",
              "0  64747.0   W_03194_50_classic_W.jpg   W_02247_50_classic_W.jpg   \n",
              "1  64747.0   W_30434_60_minimal_W.jpg  W_02498_50_feminine_W.jpg   \n",
              "2  64747.0   W_30454_60_minimal_W.jpg  W_13904_50_feminine_W.jpg   \n",
              "3  64747.0   W_35674_60_minimal_W.jpg  W_14102_50_feminine_W.jpg   \n",
              "4  64747.0  W_20598_70_military_W.jpg  W_18951_50_feminine_W.jpg   \n",
              "\n",
              "                     스타일 선호_y                  스타일 비선호_y  \n",
              "0   W_20598_70_military_W.jpg  W_02498_50_feminine_W.jpg  \n",
              "1   W_37491_70_military_W.jpg  W_14102_50_feminine_W.jpg  \n",
              "2  W_22510_80_powersuit_W.jpg   W_27828_60_minimal_W.jpg  \n",
              "3  W_46907_80_powersuit_W.jpg    W_47169_70_hippie_W.jpg  \n",
              "4     W_30988_90_kitsch_W.jpg    W_11610_90_grunge_W.jpg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-238c63fa-19f2-4bd7-87d5-34fb20c7c9f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>스타일 선호_x</th>\n",
              "      <th>스타일 비선호_x</th>\n",
              "      <th>스타일 선호_y</th>\n",
              "      <th>스타일 비선호_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64747.0</td>\n",
              "      <td>W_03194_50_classic_W.jpg</td>\n",
              "      <td>W_02247_50_classic_W.jpg</td>\n",
              "      <td>W_20598_70_military_W.jpg</td>\n",
              "      <td>W_02498_50_feminine_W.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64747.0</td>\n",
              "      <td>W_30434_60_minimal_W.jpg</td>\n",
              "      <td>W_02498_50_feminine_W.jpg</td>\n",
              "      <td>W_37491_70_military_W.jpg</td>\n",
              "      <td>W_14102_50_feminine_W.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64747.0</td>\n",
              "      <td>W_30454_60_minimal_W.jpg</td>\n",
              "      <td>W_13904_50_feminine_W.jpg</td>\n",
              "      <td>W_22510_80_powersuit_W.jpg</td>\n",
              "      <td>W_27828_60_minimal_W.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64747.0</td>\n",
              "      <td>W_35674_60_minimal_W.jpg</td>\n",
              "      <td>W_14102_50_feminine_W.jpg</td>\n",
              "      <td>W_46907_80_powersuit_W.jpg</td>\n",
              "      <td>W_47169_70_hippie_W.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64747.0</td>\n",
              "      <td>W_20598_70_military_W.jpg</td>\n",
              "      <td>W_18951_50_feminine_W.jpg</td>\n",
              "      <td>W_30988_90_kitsch_W.jpg</td>\n",
              "      <td>W_11610_90_grunge_W.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-238c63fa-19f2-4bd7-87d5-34fb20c7c9f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-238c63fa-19f2-4bd7-87d5-34fb20c7c9f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-238c63fa-19f2-4bd7-87d5-34fb20c7c9f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-640e5329-e7c8-4bfa-a3e8-ac414aaca869\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-640e5329-e7c8-4bfa-a3e8-ac414aaca869')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-640e5329-e7c8-4bfa-a3e8-ac414aaca869 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2864,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15718.273276995978,\n        \"min\": 368.0,\n        \"max\": 67975.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          22324.0,\n          61493.0,\n          59637.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc2a4\\ud0c0\\uc77c \\uc120\\ud638_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1731,\n        \"samples\": [\n          \"W_05893_90_hiphop_M.jpg\",\n          \"W_04993_70_hippie_W.jpg\",\n          \"W_06258_50_ivy_M.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc2a4\\ud0c0\\uc77c \\ube44\\uc120\\ud638_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2447,\n        \"samples\": [\n          \"W_46922_00_cityglam_W.jpg\",\n          \"W_11118_00_metrosexual_M.jpg\",\n          \"W_14581_50_feminine_W.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc2a4\\ud0c0\\uc77c \\uc120\\ud638_y\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 406,\n        \"samples\": [\n          \"W_04643_50_ivy_M.jpg\",\n          \"W_16732_70_hippie_M.jpg\",\n          \"W_04324_90_hiphop_M.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\uc2a4\\ud0c0\\uc77c \\ube44\\uc120\\ud638_y\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 590,\n        \"samples\": [\n          \"W_31439_19_normcore_M.jpg\",\n          \"W_10186_80_powersuit_W.jpg\",\n          \"W_16341_60_mods_M.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# 스타일 선호 정보표 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/DCC/DCC_스타일_선호_정보표.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imageID(x):\n",
        "    if pd.isna(x): return x\n",
        "    return x[2:7]\n",
        "\n",
        "df_t = df.copy()\n",
        "df_t[['스타일 선호_x', '스타일 비선호_x', '스타일 선호_y', '스타일 비선호_y']] = df_t[['스타일 선호_x', '스타일 비선호_x', '스타일 선호_y', '스타일 비선호_y']].applymap(imageID)"
      ],
      "metadata": {
        "id": "Rb9T0o_2a6XM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_t[['id','스타일 선호_x','스타일 비선호_x']]\n",
        "valid = df_t[['id','스타일 선호_y','스타일 비선호_y']]\n",
        "\n",
        "print(f'훈련 데이터에 존재하는 고유한 이미지 종류 : {train[\"스타일 선호_x\"].nunique()}')\n",
        "print(f'훈련 데이터에 존재하는 고유한 응답자 수 : {train.id.nunique()}')\n",
        "\n",
        "train['스타일 선호_x'] = train['스타일 선호_x'].fillna(0).astype(str)\n",
        "train['스타일 비선호_x'] = train['스타일 비선호_x'].fillna(0).astype(str)\n",
        "\n",
        "#**선호 응답 데이터 이진화**#\n",
        "pref_train = train[['id', '스타일 선호_x']].rename(columns={'스타일 선호_x': 'item'})\n",
        "pref_train['prefer'] = 1\n",
        "\n",
        "not_pref_train = train[['id', '스타일 비선호_x']].rename(columns={'스타일 비선호_x': 'item'})\n",
        "not_pref_train['prefer'] = 0"
      ],
      "metadata": {
        "id": "qnHQe7GXa8rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e61b2d-ee2d-4d22-eac2-f18cf4050225"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터에 존재하는 고유한 이미지 종류 : 1730\n",
            "훈련 데이터에 존재하는 고유한 응답자 수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 선호_x에는 1을 배정\n",
        "pref_train = train[['id', '스타일 선호_x']].rename(columns={'스타일 선호_x': 'item'})\n",
        "pref_train['prefer'] = 1\n",
        "\n",
        "# 선호_x에는 0을 배정\n",
        "not_pref_train = train[['id', '스타일 비선호_x']].rename(columns={'스타일 비선호_x': 'item'})\n",
        "not_pref_train['prefer'] = 0\n",
        "\n",
        "# 각 user, item별로 선호, 비선호 결합\n",
        "format_data = pd.concat([pref_train, not_pref_train]).reset_index(drop=True)\n",
        "\n",
        "# 유틸리티 행렬을 계산하고 결측치는 0으로 대체\n",
        "ut_matrix = format_data.pivot_table(index='id', columns='item', values='prefer', fill_value = 0)\n",
        "ut_matrix_fill_zero = ut_matrix.drop(columns='0')\n",
        "ut_matrix_fill_zero"
      ],
      "metadata": {
        "id": "sY3btu_oa-Z2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "collapsed": true,
        "outputId": "c9711cd7-ba9b-4bdf-ccbb-a69aa6aff1f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "item     00004  00007  00017  00023  00026  00027  00028  00031  00032  00034  \\\n",
              "id                                                                              \n",
              "368.0      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "837.0      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0   \n",
              "7658.0     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "7905.0     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "9096.0     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "66469.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "66513.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "66592.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "66731.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "67975.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "item     00036  00038  ...  67909  67958  68175  68199  71920  71921  71922  \\\n",
              "id                     ...                                                    \n",
              "368.0      0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "837.0      0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "7658.0     0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "7905.0     0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "9096.0     0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "...        ...    ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "66469.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "66513.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "66592.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "66731.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "67975.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "item     71923  71933  71934  71935  71936  \n",
              "id                                          \n",
              "368.0      0.0    0.0    0.0    0.0    0.0  \n",
              "837.0      0.0    0.0    0.0    0.0    0.0  \n",
              "7658.0     0.0    0.0    0.0    0.0    0.0  \n",
              "7905.0     0.0    0.0    0.0    0.0    0.0  \n",
              "9096.0     0.0    0.0    0.0    0.0    0.0  \n",
              "...        ...    ...    ...    ...    ...  \n",
              "66469.0    0.0    0.0    0.0    0.0    0.0  \n",
              "66513.0    0.0    0.0    0.0    0.0    0.0  \n",
              "66592.0    0.0    0.0    0.0    0.0    0.0  \n",
              "66731.0    0.0    0.0    0.0    0.0    0.0  \n",
              "67975.0    0.0    1.0    1.0    1.0    1.0  \n",
              "\n",
              "[100 rows x 4066 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c635bfa-17e1-404d-ac7b-4c4131fa678d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>item</th>\n",
              "      <th>00004</th>\n",
              "      <th>00007</th>\n",
              "      <th>00017</th>\n",
              "      <th>00023</th>\n",
              "      <th>00026</th>\n",
              "      <th>00027</th>\n",
              "      <th>00028</th>\n",
              "      <th>00031</th>\n",
              "      <th>00032</th>\n",
              "      <th>00034</th>\n",
              "      <th>00036</th>\n",
              "      <th>00038</th>\n",
              "      <th>...</th>\n",
              "      <th>67909</th>\n",
              "      <th>67958</th>\n",
              "      <th>68175</th>\n",
              "      <th>68199</th>\n",
              "      <th>71920</th>\n",
              "      <th>71921</th>\n",
              "      <th>71922</th>\n",
              "      <th>71923</th>\n",
              "      <th>71933</th>\n",
              "      <th>71934</th>\n",
              "      <th>71935</th>\n",
              "      <th>71936</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>368.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7658.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7905.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9096.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66469.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66513.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66592.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66731.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67975.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4066 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c635bfa-17e1-404d-ac7b-4c4131fa678d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c635bfa-17e1-404d-ac7b-4c4131fa678d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c635bfa-17e1-404d-ac7b-4c4131fa678d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0380705c-540c-4ffe-bdee-e5258cc33afb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0380705c-540c-4ffe-bdee-e5258cc33afb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0380705c-540c-4ffe-bdee-e5258cc33afb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_db71d1f7-9b17-4723-9018-073264dda96a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ut_matrix_fill_zero')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_db71d1f7-9b17-4723-9018-073264dda96a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ut_matrix_fill_zero');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ut_matrix_fill_zero"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2) Jaccard 유사도 행렬 계산**"
      ],
      "metadata": {
        "id": "0l9OtmKybBZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자카드 유사도 계산\n",
        "user_sim_ = 1 - pairwise_distances(np.array((ut_matrix_fill_zero)), metric='jaccard')\n",
        "user_sim = pd.DataFrame(user_sim_, index=ut_matrix_fill_zero.index, columns=ut_matrix_fill_zero.index)\n",
        "\n",
        "item_sim_ = 1 - pairwise_distances(np.array((ut_matrix_fill_zero.T)), metric='jaccard')\n",
        "item_sim = pd.DataFrame(item_sim_, index = ut_matrix_fill_zero.columns, columns = ut_matrix_fill_zero.columns)\n",
        "\n",
        "# 아이템간의 자카드 유사도 행렬\n",
        "print(user_sim.shape) # 100 X 100\n",
        "print(item_sim.shape) # 4066 X 4066"
      ],
      "metadata": {
        "id": "nvxBj0KkbDeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0195b9ff-8cbc-49a9-db1a-08fa05094287"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py:2361: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py:2361: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 100)\n",
            "(4066, 4066)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3) 단일 사용자-아이템 쌍에 대한 협업 필터링 수행**"
      ],
      "metadata": {
        "id": "CEvuhLSzbGl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_user_preference(user_id, item_id, k=3, th=0.5):\n",
        "  '''\n",
        "  사용자의 자카드 유사도 행렬에서 입력한 user_id와 유사한 k개의 user_id 선택\n",
        "  선택된 user_id를 이용해서 입력한 item_id의 선호도 평점 예측\n",
        "  지정한 threshold보다 높으면 선호, 낮으면 비선호로 예측한다.\n",
        "\n",
        "  user_id : 사용자 식별 id\n",
        "  item_id : 아이템 식별 id\n",
        "  k : 유사도 선택 개수\n",
        "  th : 선호도 판단 기준\n",
        "\n",
        "  ut_matrix_fill_zero : 유틸리티 행렬\n",
        "  user_sim : 사용자 유사도 행렬\n",
        "  similar_users : 유사도가 가장 높은 k명의 유저\n",
        "  '''\n",
        "  us = user_sim[user_id]\n",
        "  similar_users = us.drop(index=user_id).nlargest(k).index # 자기 자신과의 유사도는 제외\n",
        "  pref = ut_matrix_fill_zero.loc[similar_users, str(item_id)]\n",
        "  avg_pref = pref.mean() # 유사도가 가장 높은 k user의 아이템 선호도의 평균\n",
        "\n",
        "  prediction = 1 if avg_pref >= th else 0\n",
        "\n",
        "  # 추천의 근거\n",
        "  print(f'사용자 기반 추천의 근거 : 유사도가 높은 {similar_users.values}의 응답이 반영')\n",
        "\n",
        "  return prediction\n",
        "\n",
        "def predict_item_preference(user_id, item_id, k=3, th=0.5):\n",
        "  '''\n",
        "  아이템의 자카드 유사도 행렬에서 입력한 item_id와 유저가 평가한 유사한 k개의 item_id 선택\n",
        "  선택된 kr개의 item_id에 대한 선호도 평가를 이용해서 입력한 item_id의 선호도 평점 예측\n",
        "  지정한 threshold보다 높으면 선호, 낮으면 비선호로 예측한다.\n",
        "\n",
        "  user_id : 사용자 식별 id\n",
        "  item_id : 아이템 식별 id\n",
        "  k : 유사도 선택 개수\n",
        "  th : 선호도 판단 기준\n",
        "\n",
        "  ut_matrix_fill_zero : 유틸리티 행렬\n",
        "  item_sim : 아이템 유사도 행렬\n",
        "  similar_items : 유사도가 가장 높은 k명의 아이템\n",
        "  '''\n",
        "  item_simil = item_sim[item_id]\n",
        "  similar_items = item_simil.drop(index=item_id).nlargest(k).index\n",
        "  pref = ut_matrix_fill_zero.loc[user_id, similar_items]\n",
        "  avg_pref = pref.mean()\n",
        "\n",
        "  prediction = 1 if avg_pref >= th else 0\n",
        "\n",
        "  # 추천의 근거\n",
        "  print(f'아이템 기반 추천의 근거 : 유사도가 높은 {similar_items.values}에 대한 선호도가 반영')\n",
        "\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "hLt43hW0bKmz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## user_id가 837인 사용자의 item_id : 68175 선호도 예측\n",
        "user_preds = predict_user_preference(user_id=837, item_id='68175', k=5, th=0.5)\n",
        "print(f'사용자 기반 협업 필터링 결과 : {user_preds}')\n",
        "\n",
        "item_preds = predict_item_preference(user_id=837, item_id='68175', k=3, th=0.5)\n",
        "print(f'아이템 기반 협업 필터링 결과 : {item_preds}')"
      ],
      "metadata": {
        "id": "xsvWgBgIbOtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcdc84bc-d97d-4ff8-c45f-4cde270f64e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자 기반 추천의 근거 : 유사도가 높은 [60184. 28371. 62349. 63748. 64345.]의 응답이 반영\n",
            "사용자 기반 협업 필터링 결과 : 0\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01933' '08913' '13018']에 대한 선호도가 반영\n",
            "아이템 기반 협업 필터링 결과 : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이처럼 사용자 기반 협업 필터링은 추천의 이유를 익명화된 사용자의 응답을 바탕으로 제공하기 때문에 추천에 대한 신뢰도가 상대적으로 낮다. 하지만 아이템 기반 협업 필터링은 아이템이 가지는 특징을 기반으로 추천을 제시하고 유사도가 높은 아이템을 확인할 수 있기 때문에 신뢰도가 상대적으로 높다는 장점이 존재한다."
      ],
      "metadata": {
        "id": "gDIQJogKbUCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4) 사용자/아이템 기반 협업 필터링 수행**"
      ],
      "metadata": {
        "id": "TrwPslSZbXGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터와 동일한 전처리 진행\n",
        "valid['스타일 선호_y'] = valid['스타일 선호_y'].fillna(0).astype(str)\n",
        "valid['스타일 비선호_y'] = valid['스타일 비선호_y'].fillna(0).astype(str)\n",
        "\n",
        "pref_valid = valid[['id', '스타일 선호_y']].rename(columns={'스타일 선호_y': 'item'})\n",
        "pref_valid['prefer'] = 1\n",
        "pref_v = pref_valid[pref_valid['item'] != '0']\n",
        "\n",
        "not_pref_valid = valid[['id', '스타일 비선호_y']].rename(columns={'스타일 비선호_y': 'item'})\n",
        "not_pref_valid['prefer'] = 0\n",
        "not_pref_v = not_pref_valid[not_pref_valid['item'] != '0']\n",
        "\n",
        "# id와 item 관계를 딕셔너리로 표현\n",
        "pref_dict = pref_v.groupby('id')['item'].apply(list).to_dict()\n",
        "not_pref_dict = not_pref_v.groupby('id')['item'].apply(list).to_dict()"
      ],
      "metadata": {
        "id": "V606vfCjbakP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "선호하는 아이템 중에서 예측 결과 확인 - True 값이 모두 1"
      ],
      "metadata": {
        "id": "R56PB_6XbdVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_cnt, it_cnt = [],[]\n",
        "\n",
        "missing_keys = []\n",
        "missing_values = []\n",
        "\n",
        "for key,value in pref_dict.items():\n",
        "  for id in value:\n",
        "    try:\n",
        "      user_preds = predict_user_preference(user_id=key, item_id=id, k=3, th=0.5)\n",
        "      item_preds = predict_item_preference(user_id=key, item_id=id, k=3, th=0.5)\n",
        "    except KeyError:\n",
        "      missing_keys.append(key)\n",
        "      missing_values.append(id)\n",
        "      continue\n",
        "    us_cnt.append(user_preds)\n",
        "    it_cnt.append(item_preds)\n",
        "\n",
        "us_result = sum(us_cnt)/(pref_v.shape[0] - len(missing_values)) * 100\n",
        "it_result = sum(it_cnt)/(pref_v.shape[0] - len(missing_values)) * 100\n",
        "\n",
        "sol = [1] * (pref_v.shape[0] - len(missing_values))"
      ],
      "metadata": {
        "id": "Ayn9M7KubfVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2c8b05-6bc2-4963-bd1e-b9fb4e330ce5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00799' '00886' '01703']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00799' '00886' '02804']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00495' '00500' '00540']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['28133' '00031' '00799']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [60184. 28371. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01520' '01568' '01706']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [60184. 28371. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00811' '01520' '01568']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [60184. 28371. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00843' '01520' '01568']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59637.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04128' '06118' '06322']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '01424' '01474']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00564' '01075' '02085']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00564' '02085' '03160']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['05019' '05227' '05345']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04639' '09183' '09758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04639' '09183' '09758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00833' '01645' '01716']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00555' '01670']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01532' '01658' '01804']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00803' '01532']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04638' '17461' '24579']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04638' '15402' '17461']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['15402' '17461' '24579']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04638' '04707' '06514']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63481.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00813' '00866' '01480']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 21432. 64295.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00833' '01645' '01716']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 21432. 64295.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00833' '01645' '01716']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 21432. 64295.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00047' '00833' '00856']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63435.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01041' '01166' '01212']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63435.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01166' '01259' '04996']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63435.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01166' '01259' '03992']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00101' '00889' '01704']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00101' '00889' '01704']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00101' '00889' '01704']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424. 61104.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02265' '03699' '03717']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424. 61104.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02265' '03699' '03717']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424. 61104.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02265' '03699' '03807']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424. 61104.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02234' '02265' '02274']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00332' '03476' '03673']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00332' '03476' '03673']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7658. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00784' '02106' '03580']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '00625' '01367']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '00625' '01367']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00625' '01367' '01395']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '00625' '01367']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '01367' '01395']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '00625' '01367']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00489' '00555']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00803' '01571']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '00489' '00803']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00803' '01571']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00507' '00803']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00803' '01532']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913. 63392.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00838' '06671' '06672']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00729' '01268' '01380']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00152' '00729' '01268']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00152' '00729' '01268']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00004' '00924' '01687']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00924' '01687' '06165']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 61493. 64460.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01693' '02762' '02818']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 61493. 64460.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01693' '02762' '06829']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01731' '01741' '01758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01731' '01741' '01758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01731' '01741' '01758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02234' '02274' '02487']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02234' '02274' '02487']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02234' '02265' '02274']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['08913' '13018' '13173']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01933' '13018' '13173']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01933' '08913' '13018']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01693' '02762' '02818']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02818' '02881' '04661']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02818' '02881' '04630']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02818' '02881' '04630']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01106' '01183' '02567']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00027' '01490' '01539']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [  368. 64441.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01394' '01752' '02693']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [  368. 64441.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01394' '01752' '02693']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00500' '00951' '01561']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00495' '00500' '00540']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00437' '01084' '01275']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00437' '01084' '01275']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00760' '01207' '01379']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00760' '01207' '01379']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00760' '01207' '01379']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63156. 63601.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01998' '01999' '02258']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63156. 63601.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01318' '01998' '01999']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63156. 63601.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01999' '02258' '02508']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63156. 63601.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01998' '01999' '02258']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63156. 63601.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01998' '01999' '02258']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00812' '02958' '02983']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01429' '01810' '02878']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01429' '01810' '02891']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04928' '20774' '22563']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01899' '02023' '02150']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01899' '02023' '02150']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01899' '02023' '02150']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01318' '01998' '01999']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00940' '01737' '02771']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01997' '05594' '06484']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63583. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00410' '00432' '00559']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63583. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00410' '01208' '01389']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63583. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00410' '01208' '01958']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63583. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00410' '01208' '01389']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00507' '00803']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00507' '00848' '01428']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00079' '00828' '02844']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00079' '00828' '02844']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00079' '00828' '02844']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00079' '00828' '02844']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00489' '00555']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00555' '01670']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00555' '01670']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00555' '01670']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506. 62525.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01962' '01989' '04842']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506. 62525.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01962' '01989' '04842']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00570' '00704' '01927']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00570' '00704' '01927']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [58251.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01041' '01166' '01212']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [58251.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01041' '01212' '02585']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [58251.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01041' '01212' '02585']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [58251.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01041' '01212' '02585']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03601' '03933' '05254']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03601' '03933' '05254']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59637. 59642.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00333' '00580' '00974']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59637. 59642.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00333' '00580' '00974']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59637. 59642.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01329' '00333' '00580']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59637. 59642.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00333' '00580' '01371']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [28571. 64633.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01480' '01636' '03033']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [28571. 64633.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00813' '00866' '01480']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [28571. 64633.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['28133' '00031' '00799']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7905. 35514. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '01424' '01474']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7905. 35514. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '01424' '01474']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7905. 35514. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '00489' '00803']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7905. 35514. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '01424' '01474']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7905. 35514. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '01424' '01474']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 60234. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04562' '06917' '07366']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 60234. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04562' '07366' '10135']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 60234. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00023' '00555' '01670']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00410' '00432' '00559']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00432' '00559' '01993']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00432' '00559' '01993']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00432' '00559' '01993']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00432' '00559' '01993']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00432' '00559' '01993']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00682' '02230' '02378']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00682' '02230' '02378']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00682' '02230' '02378']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62952.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01188' '01378' '03457']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00385' '02100' '03569']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00385' '02100' '03663']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00385' '02100' '03569']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00162' '00194' '01116']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00162' '00194' '01116']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00162' '00194' '01116']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03092' '03118' '03934']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03092' '03118' '03934']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03092' '03118' '03934']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03092' '03118' '03934']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00400' '01179' '01321']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00400' '01179' '01321']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00400' '01321' '02009']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00811' '06246' '07090']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00811' '01520' '01568']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00811' '06246' '07090']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01329' '00333' '00580']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00514' '02779' '02846']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00514' '02779' '02846']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64364.   368. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00495' '00540' '01595']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64364.   368. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00495' '01595' '01666']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64364.   368. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00495' '00500' '00540']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00543' '00553' '01477']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00901' '02900' '04253']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62155. 64221. 64364.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01539' '01558' '02711']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62155. 64221. 64364.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00027' '01490' '01539']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00474' '04279' '05866']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02966' '04217' '06591']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02816' '02966' '04217']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01617' '04537' '06631']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01617' '04537' '06631']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01617' '04537' '06631']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00032' '00947' '05883']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02710' '06234' '09280']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02878' '03013' '10098']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01429' '01810' '02878']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02878' '03013' '10098']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00935' '02852' '04692']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62653. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00479' '00805' '01413']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62653. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00479' '00805' '01413']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62653. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00479' '00805' '01413']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62653. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00479' '00805' '01413']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00843' '12479' '15856']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00843' '12479' '15295']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00843' '12479' '15295']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00843' '01520' '01568']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00047' '00856' '02754']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00047' '00856' '02754']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00047' '00833' '00856']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00047' '00856' '02754']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 63930.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00824' '01472' '02918']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01563' '02953' '07344']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01563' '02660' '02953']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04506' '12834' '17015']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04506' '12834' '17015']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04506' '12834' '17015']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64221. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04707' '06514' '10792']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64221. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04707' '06514' '10792']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64221. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['28133' '00031' '00799']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64221. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04638' '04707' '06514']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63644.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['07755' '08808' '20414']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63644.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['07755' '08808' '20414']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63644.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['07755' '08808' '20414']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06046' '18066' '18759']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06046' '18066' '18205']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06046' '18066' '18205']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06046' '18066' '18205']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['18066' '18205' '18759']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06046' '18066' '18205']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00017' '00071' '00794']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00017' '00071' '00794']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00638' '02024' '02431']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00638' '02024' '02431']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00031' '02888' '15182']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00031' '02888' '15182']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00031' '02888' '15182']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00031' '02888' '15182']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['28133' '00031' '00799']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['08270' '08691' '11303']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['08270' '08691' '11303']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['08270' '11303' '21314']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03194' '04972' '05628']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03194' '04972' '05628']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00253' '00893' '02343']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03194' '04972' '07894']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['03194' '04972' '05628']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06684' '15891' '16523']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00456' '00486' '00588']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00456' '00486' '00588']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00486' '00588' '01322']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00456' '00486' '00588']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00456' '00486' '00588']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02244' '05088' '07416']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00253' '00770' '00893']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00893' '02343' '02820']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00253' '00893' '02343']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00253' '00893' '02343']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06922' '07074' '07095']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06922' '07095' '17738']에 대한 선호도가 반영\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "비선호하는 아이템 중에서 예측 결과 확인 - True 값이 모두 0"
      ],
      "metadata": {
        "id": "V2Boigazbime"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_cnt_v, it_cnt_v = [], []\n",
        "\n",
        "missing_keys_v = []\n",
        "missing_values_v = []\n",
        "\n",
        "for key,value in not_pref_dict.items():\n",
        "  for id in value:\n",
        "\n",
        "    try:\n",
        "      user_preds = predict_user_preference(user_id=key, item_id=id, k=3, th=0.5)\n",
        "      item_preds = predict_item_preference(user_id=key, item_id=id, k=3, th=0.5)\n",
        "    except KeyError:\n",
        "      missing_keys_v.append(key)\n",
        "      missing_values_v.append(id)\n",
        "      continue\n",
        "    us_cnt_v.append(user_preds)\n",
        "    it_cnt_v.append(item_preds)\n",
        "\n",
        "us_result_v = sum(us_cnt_v)/(not_pref_v.shape[0] - len(missing_values_v)) * 100\n",
        "it_result_v = sum(it_cnt_v)/(not_pref_v.shape[0] - len(missing_values_v)) * 100\n",
        "\n",
        "sol_v = [0] * (not_pref_v.shape[0] - len(missing_values_v))"
      ],
      "metadata": {
        "id": "0zmvqo_ybiT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9146c894-e457-48eb-c841-fb819c070fab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01532' '01658' '01804']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63571. 28571. 63910.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [60184. 28371. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [60184. 28371. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59637.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59637.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59637.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59637.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59637.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63505.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 63927. 35514.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63769. 64252. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460.   837.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63481.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63481.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63481.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63481.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63481.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00843' '12479' '15856']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01394' '01752' '02693']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 21432. 64295.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 21432. 64295.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01617' '04537' '06631']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 21432. 64295.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63435.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63435.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63435.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 61493. 63913.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424. 61104.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7658. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7658. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7658. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7658. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63740. 63479.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324. 63369. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913. 63392.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913. 63392.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913. 63392.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913. 63392.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913. 63392.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 61493. 64460.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 61493. 64460.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 61493. 64460.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00543' '00553' '01477']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00479' '00805' '01413']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['28133' '00031' '00799']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00489' '00803' '01571']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61859. 21432. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00813' '00866' '01480']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01999' '02258' '02508']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [  368. 64441.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [  368. 64441.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [  368. 64441.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63424.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63156. 63601.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00004' '00924' '01687']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00479' '00805' '01413']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64336.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01480' '01636' '03033']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64295. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63569.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00833' '01645' '01716']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00760' '01207' '01379']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63583. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00760' '01207' '01379']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63583. 63526.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59704. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00475' '01424' '01474']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63508. 21432. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506. 62525.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506. 62525.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506. 62525.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59506. 62525.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02265' '03699' '03807']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [58251.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [58251.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59637. 59642.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02265' '03699' '03807']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59637. 59642.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [28571. 64633.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [28571. 64633.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [28571. 64633.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 7905. 35514. 59704.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 60234. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 60234. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432. 60234. 63405.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359. 62361.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62952.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62952.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62952.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62952.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62952.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63359.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02265' '03699' '03807']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62625.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01998' '01999' '02258']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64503.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00400' '01179' '01321']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '01367' '01395']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63759. 59642.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00332' '03476' '03673']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01480' '01636' '03033']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [61493. 63405.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63479. 63740.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64364.   368. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64364.   368. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64364.   368. 62349.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00811' '06246' '07090']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [59812. 63934. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [21432.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62155. 64221. 64364.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62155. 64221. 64364.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62155. 64221. 64364.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63913.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930. 64460.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930. 64460.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930. 64460.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04562' '06917' '07366']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930. 64460.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63930. 64460.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01731' '01741' '01758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02966' '04217' '06591']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04639' '09183' '09758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00031' '02888' '15182']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [22324.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['02878' '03013' '10098']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62868. 35514.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [62653. 63505.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 837.  368. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [35514. 64441.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 63930.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 63930.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 63930.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 63930.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63910. 63930.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01617' '04537' '06631']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00253' '00893' '02343']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 61493. 59083.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04707' '06514' '10792']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64633. 64221. 60234.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63644.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63644.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [63644.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['06922' '07074' '07095']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [64460. 63481. 62653.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['07755' '08808' '20414']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['04506' '12834' '17015']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00366' '00625' '01367']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66731. 64747.   368.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [66592.   368.   837.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['01731' '01741' '01758']에 대한 선호도가 반영\n",
            "사용자 기반 추천의 근거 : 유사도가 높은 [ 368.  837. 7658.]의 응답이 반영\n",
            "아이템 기반 추천의 근거 : 유사도가 높은 ['00026' '00034' '00036']에 대한 선호도가 반영\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5) 최종 결과 확인 (훈련 데이터의 사용자가 응답하지 않은 아이템에 대해선 선호도를 예측할 수 없음)**"
      ],
      "metadata": {
        "id": "oG3ySgNZbsNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_prediction = us_cnt + us_cnt_v\n",
        "it_prediction = it_cnt + it_cnt_v\n",
        "solution = sol + sol_v\n",
        "\n",
        "print(f' 사용자 기반 혼동 행렬 \\n {confusion_matrix(solution, us_prediction)}')\n",
        "print(f' 아이템 기반 혼동 행렬 \\n {confusion_matrix(solution, it_prediction)}\\n')\n",
        "\n",
        "print(f' 사용자 기반 정확도 : {accuracy_score(solution, us_prediction)}')\n",
        "print(f' 아이템 기반 정확도 : {accuracy_score(solution, it_prediction)}\\n')\n",
        "\n",
        "print(f' 사용자 기반 재현율 : {recall_score(solution, us_prediction)}')\n",
        "print(f' 아이템 기반 재현율 : {recall_score(solution, it_prediction)}\\n')\n",
        "\n",
        "print(f' 사용자 기반 정밀도 : {precision_score(solution, us_prediction)}')\n",
        "print(f' 아이템 기반 정밀도 : {precision_score(solution, it_prediction)}\\n')\n",
        "\n",
        "print(f' 사용자 기반 F1 스코어 : {f1_score(solution, us_prediction)}')\n",
        "print(f' 아이템 기반 F1 스코어 : {f1_score(solution, it_prediction)}\\n')\n",
        "\n",
        "print(f'사용자 기반 협업 필터링 누락 값 {missing_values}')"
      ],
      "metadata": {
        "id": "gIEvXCEHbtmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83ca90d-ec9e-43ec-f9a2-5a2d96d88529"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 사용자 기반 혼동 행렬 \n",
            " [[402   0]\n",
            " [241   6]]\n",
            " 아이템 기반 혼동 행렬 \n",
            " [[401   1]\n",
            " [ 29 218]]\n",
            "\n",
            " 사용자 기반 정확도 : 0.6286594761171033\n",
            " 아이템 기반 정확도 : 0.9537750385208013\n",
            "\n",
            " 사용자 기반 재현율 : 0.024291497975708502\n",
            " 아이템 기반 재현율 : 0.8825910931174089\n",
            "\n",
            " 사용자 기반 정밀도 : 1.0\n",
            " 아이템 기반 정밀도 : 0.9954337899543378\n",
            "\n",
            " 사용자 기반 F1 스코어 : 0.04743083003952569\n",
            " 아이템 기반 F1 스코어 : 0.9356223175965666\n",
            "\n",
            "사용자 기반 협업 필터링 누락 값 ['06864', '00551', '10104', '06590', '04927', '09731', '32034', '14783', '18714', '19205', '06522', '00931', '07120', '24770', '38585', '57473', '50836', '26393', '16375', '10103', '00851', '15758', '02705', '19520', '00716', '14785', '06190', '04629', '09285', '16466', '05851', '09750', '11778', '01294', '14572', '08977', '13251', '05917', '08492', '07643', '02394', '05716', '14706', '15244', '06609', '02770', '12789', '00842', '02908', '09085', '08951', '01387', '10184', '29347', '28373', '34504', '44789', '47408', '62253', '17235', '09277', '03003', '00012', '06358', '10251', '09747', '17004', '00804', '09278', '09889', '10167', '02306', '02223', '10558', '14691', '19267', '10253', '07916', '03334', '14785', '12880', '06525', '16704', '12412', '45137', '47862', '01035', '03447', '14796', '05175', '05791', '06498', '00161', '01000', '06525', '00865', '06576', '02677', '02879', '06860', '08641', '10598', '01056', '02557', '13277', '02275', '02557', '01896', '09085', '06438', '07871', '07238', '02837', '02691', '09278', '00511', '12518', '12789', '17697', '06502', '03685', '03531', '14785', '18058', '14512', '03732', '04845', '08951', '13781', '01925', '06453', '03859', '08621', '09475', '09737', '09577', '05979', '01410', '17366', '00033', '16851', '01552', '17239', '05892', '02701', '17854', '07018', '27040', '15662', '28925', '25086', '28453', '10783', '12668', '28719', '00117', '02688', '12567', '12530', '28563', '33329', '12847', '28022', '07316', '04670', '26200', '09881', '16067', '17783', '16732', '23899', '25649', '32385', '08754', '08988', '33305', '41448', '34952', '41341', '47967', '16690', '06797', '37491', '30988', '39164', '38588', '38785', '38489', '34621', '07990', '53808', '01123', '52417', '21988', '52578']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아이템 기반 협업 필터링은 선호도를 예측하고자 하는 사용자의 응답을 반영하기 때문에 사용자 기반 협업 필터링보다 선호도를 더 정확하게 예측할 가능성이 높다.\n",
        "\n",
        "현재 보유한 데이터는 응답자 수가 100명이고 응답한 아이템의 종류는 약 4000개 이다. 동일한 스타일 클래스로 아이템을 구분하지 않고 독립적인 아이템 id를 기준으로 구분하기 때문에 사용자의 응답에 따른 사용자 유사도를 정확하게 계산하기 구조이다. 하지만 상대적으로 아이템과 관련된 데이터는 많고 아이템 기반 협업 필터링이 가지는 장점 덕분에 아이템 기반 협업 필터링이 사용자 기반 협업 필터링 보다 우수한 성능을 보였다."
      ],
      "metadata": {
        "id": "UaynJfpVbx0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **미션 3-2**"
      ],
      "metadata": {
        "id": "Rr69qeyLySS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train, valid 데이터 불러오기\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/DCC/df_train_response.csv')\n",
        "df_valid = pd.read_csv('/content/drive/MyDrive/DCC/df_valid_response.csv')"
      ],
      "metadata": {
        "id": "sfVl_la8CYM4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rating Matrix 생성 & Valid data"
      ],
      "metadata": {
        "id": "mgRisAkkDLT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1, Q5의 조합을 이용해서 응답 결과 수정"
      ],
      "metadata": {
        "id": "A2YjmxS3CYiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train, valid 데이터에 대하여 설문 응답 결과 중 사용할 결과 데이터 불러오기\n",
        "\n",
        "df_train_t = df_train[['E_id','R_id','imgName', 'era', 'style', 'gender', 'Q1', 'Q2', 'Q3', 'Q411', 'Q412', 'Q413', 'Q414', 'Q4201',\n",
        "       'Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209',\n",
        "       'Q4210', 'Q4211', 'Q4212', 'Q4213', 'Q4214', 'Q4215', 'Q4216', 'Q5']]\n",
        "df_valid_t = df_valid[['E_id','R_id','imgName', 'era', 'style', 'gender', 'Q1', 'Q2', 'Q3', 'Q411', 'Q412', 'Q413', 'Q414', 'Q4201',\n",
        "       'Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209',\n",
        "       'Q4210', 'Q4211', 'Q4212', 'Q4213', 'Q4214', 'Q4215', 'Q4216', 'Q5']]\n",
        "\n",
        "df_train_t['img_id'] = df_train_t['imgName'].str[2:7]\n",
        "df_valid_t['img_id'] = df_valid_t['imgName'].str[2:7]"
      ],
      "metadata": {
        "id": "gr55s9CHCdKo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지에 대한 선호 여부 관련 데이터 불러오기\n",
        "df_train_t1 = df_train_t[['img_id','R_id','Q1','Q5']]\n",
        "df_valid_t1 = df_valid_t[['img_id','R_id','Q1','Q5']]"
      ],
      "metadata": {
        "id": "Hw-lR7b5CiR1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_values(row):\n",
        "  '''\n",
        "  Q1과 Q5의 응답 결과를 조합하여 응답 결과의 강도를 조절하는 함수\n",
        "  Q5의 응답의 방향을 따라가되, Q1의 응답에 따라서 강도를 동일한 간격으로 조절한다.\n",
        "  이때, 극도로 응답이 변하는 경우는 향후 선호도 예측에 방해가 된다고 판단되어 제거할 계획\n",
        "\n",
        "  start_res : Q1의 응답 결과\n",
        "  last_res : Q5의 응답 결과\n",
        "  new_res : 조합된 응답 결과\n",
        "  '''\n",
        "  start_res = row.iloc[-2]  # 응답 시작시 선호도 (Q1)\n",
        "  last_res = row.iloc[-1]  # 응답 마무리시 선호도 (Q5) 선호 - 2, 비선호 - 1\n",
        "\n",
        "  # 복잡한 조건 작성 (예시)\n",
        "  if start_res == 4 and last_res == 1: #매우 선호 & 비선호 - 삭제\n",
        "    return -0.25\n",
        "  elif start_res == 4 and last_res == 2: #매우 선호 & 선호\n",
        "    return 1\n",
        "  elif start_res == 3 and last_res == 1: #선호 & 비선호\n",
        "    return -0.5\n",
        "  elif start_res == 3 and last_res == 2: #선호 & 선호\n",
        "    return 0.75\n",
        "  elif start_res == 2 and last_res == 1: #비선호 & 비선호\n",
        "    return -0.75\n",
        "  elif start_res == 2 and last_res == 2: #비선호 & 선호\n",
        "    return 0.5\n",
        "  elif start_res == 1 and last_res == 1: #매우 비선호 & 비선호\n",
        "    return -1\n",
        "  elif start_res == 1 and last_res == 2: #매우 비선호 & 선호 - 삭제\n",
        "    return 0.25\n",
        "\n",
        "def valid_compare(row):\n",
        "  '''\n",
        "  Valid에 존재하는 응답결과 1, 2를 선호도 예측을 위한 값으로 변환\n",
        "\n",
        "  last_res : Q5의 응답 결과\n",
        "  '''\n",
        "  last_res = row.iloc[-1]  # 응답 마무리시 선호도 (Q5) 선호 - 2, 비선호 - 1\n",
        "  if last_res == 1: #비선호\n",
        "    return -1\n",
        "  elif last_res == 2: #선호\n",
        "    return 1\n",
        "\n",
        "# 함수 적용\n",
        "df_train_t1['new_res'] = df_train_t1.apply(compare_values, axis=1)\n",
        "df_valid_t1['new_res'] = df_valid_t1.apply(valid_compare, axis=1)"
      ],
      "metadata": {
        "id": "XAmit9btCkWo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결측치 제거 - (Q1 매우 비선호 & Q5 선호),(Q1 매우 선호 & Q5 비선호)"
      ],
      "metadata": {
        "id": "S6veYBIbCnzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_na1 = df_train_t1[df_train_t1.new_res != -0.25]\n",
        "df_train_na2 = df_train_na1[df_train_na1.new_res != 0.25]\n",
        "\n",
        "df_train_t1 = df_train_na2.copy()"
      ],
      "metadata": {
        "id": "KsptLUdoCmJZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_df_train = df_train_t1.R_id.value_counts().reset_index(name='count') # 빈도 수 계산\n",
        "id_df_valid = df_valid_t1.R_id.value_counts().reset_index(name='count') # 빈도 수 계산\n",
        "\n",
        "# id_df_train와 id_df_valid id를 기준으로 결합\n",
        "id_counts = id_df_train.merge(id_df_valid, on='R_id', how='outer')\n",
        "\n",
        "# id별로 train/valid의 응답 합계 계산\n",
        "id_counts['count'] = id_counts['count_x'].fillna(0) + id_counts['count_y'].fillna(0)\n",
        "\n",
        "# 합계 컬럼만 남기고 응답 수 상위 100명의 id만 남겨 내림차순 정렬\n",
        "id_counts = id_counts.drop(columns=['count_x', 'count_y']).sort_values(by='count', ascending=False).nlargest(100, 'count')\n",
        "id_list = id_counts.R_id.values\n",
        "\n",
        "df_train_t11 = df_train_t1[df_train_t1.R_id.isin(id_list)]\n",
        "df_valid_t11 = df_valid_t1[df_valid_t1.R_id.isin(id_list)]"
      ],
      "metadata": {
        "id": "bHqghpMWCwst"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_t11.rename(columns={'R_id':'id','new_res':'rating','img_id':'image_id'},inplace=True)\n",
        "df_valid_t11.rename(columns={'R_id':'id','new_res':'rating','img_id':'image_id'},inplace=True)\n",
        "\n",
        "df_train_t11['image_id'] = df_train_t11['image_id'].astype(str).str.zfill(5)"
      ],
      "metadata": {
        "id": "eCDWkLIWC6UY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#상위 100명의 ID를 구해온다.\n",
        "rating_matrix = df_train_t11.pivot_table(index='id', columns='image_id', values='rating',aggfunc='mean')\n",
        "valid_df_t = df_valid_t11[['id','rating','image_id']]"
      ],
      "metadata": {
        "id": "u6go0UgiDAmH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Valid 설문 결과 사용하지 않고 예측**"
      ],
      "metadata": {
        "id": "rNLxUFHjo109"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 피처 벡터 & 유사도 생성"
      ],
      "metadata": {
        "id": "rlbqgoXnu7iC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 이미지별 응답 결과를 최빈값 기준으로 병합\n",
        "##### 이때 Valid 데이터에 존재하는 응답은 사용하지 않는다\n",
        "##### Valid에만 존재하는 이미지 응답 결과를 대체하기 위해 기존의 유사도 행렬 기준으로 가장 유사한 이미지의 응답 결과를 가져온다"
      ],
      "metadata": {
        "id": "EACyKlg0DPQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_list = ['Q1', 'Q2', 'Q3', 'Q411', 'Q412', 'Q413', 'Q414', 'Q4201','Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209','Q4210', 'Q4211', 'Q4212', 'Q4213', 'Q4214', 'Q4215', 'Q4216']\n",
        "\n",
        "df_train_t2 = df_train_t[['img_id'] + question_list]\n",
        "df_valid_t2 = df_valid_t[['img_id'] + question_list]\n",
        "\n",
        "df_ft = df_train_t2.copy()"
      ],
      "metadata": {
        "id": "D4dL5NpKDIdp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train 설문지에 존재하는 이미지 리스트\n",
        "img_id_list = df_train_t2.img_id.unique().tolist()\n",
        "print(len(img_id_list))\n",
        "\n",
        "#Valid 설문지에 존재하는 이미지 리스트\n",
        "img_id_list_v = df_valid_t2.img_id.unique().tolist()\n",
        "print(len(img_id_list_v))\n",
        "print(len(set(img_id_list + img_id_list_v)))\n",
        "\n",
        "#Valid에만 재하는 이미지 리스트존\n",
        "inter_valid = list(set(img_id_list_v) - set(img_id_list))\n",
        "print(len(inter_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK-seSY2DUO5",
        "outputId": "1f34294c-4347-4ea9-f75a-9ac165280044"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4066\n",
            "951\n",
            "4486\n",
            "420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mode(series):\n",
        "  '''\n",
        "  특정 이미지에 대한 응답을 최빈값을 이용해서 정리하는 함수\n",
        "  만일 동일한 빈도 수의 응답이 존재하면 random으로 하나를 선택한다.\n",
        "  '''\n",
        "  np.random.seed(42)\n",
        "  mode = series.mode()\n",
        "  if len(mode) > 1: return np.random.choice(mode)\n",
        "  return mode[0]\n",
        "\n",
        "\n",
        "grouped = df_ft.groupby('img_id').agg(lambda x: calculate_mode(x) if x.name != 'img_id' else x.iloc[0])"
      ],
      "metadata": {
        "id": "tzR7UKqXDwQn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "응답 결과에 따른 유사도를 구하기 위해서 다양한 응답 결과가 존재하는 설문을 원-핫 인코딩을 수행. 또한 이진 분류 응답임에도 두 응답 모두 유의미한 응답이라면 원-핫 인코딩 수행"
      ],
      "metadata": {
        "id": "-xAv9YH-VGDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2,Q3, Q411, Q412, Q413, Q414 원핫 인코딩 수행\n",
        "grouped_one = pd.get_dummies(grouped, columns=['Q2', 'Q3','Q411','Q412','Q413','Q414'],dtype=int)\n",
        "\n",
        "#이진 변수들 0,1 이진 변수화 진행\n",
        "fin_group = grouped_one[['Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209', 'Q4210',\n",
        "                   'Q4211','Q4212','Q4213','Q4214', 'Q4215', 'Q4216', 'Q2_1', 'Q2_2', 'Q2_3', 'Q3_1',\n",
        "                   'Q3_2', 'Q3_3', 'Q3_4', 'Q3_5', 'Q3_6', 'Q3_7', 'Q3_8', 'Q411_1', 'Q411_2', 'Q411_3','Q412_1', 'Q412_2', 'Q413_1',\n",
        "       'Q413_2', 'Q414_1', 'Q414_2']].applymap(lambda x: 1 if x != 0 else 0)\n",
        "fin_group.index = fin_group.index.astype(str)"
      ],
      "metadata": {
        "id": "jfL7nYoRD1M4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fin_group 이미지 단어 변수 그룹화. 비슷한 의미를 가지는 단어들끼리 그룹화\n",
        "\n",
        "fin_group_new = fin_group.copy()\n",
        "fin_group_new['new_1'] =(fin_group_new['Q4202'] + fin_group_new['Q4204'] + fin_group_new['Q4215'])\n",
        "fin_group_new['new_2'] = (fin_group_new['Q4205'] + fin_group_new['Q4208'] + fin_group_new['Q4216'])\n",
        "fin_group_new['new_3'] = (fin_group_new['Q4206'] + fin_group_new['Q4207'] + fin_group_new['Q4209'])\n",
        "fin_group_new['new_4'] = (fin_group_new['Q4210'] + fin_group_new['Q4211'] + fin_group_new['Q4212'] )\n",
        "fin_group_new['new_5'] = (fin_group_new['Q4213'] + fin_group_new['Q4214'])\n",
        "\n",
        "#그룹화 된 변수들은 drop\n",
        "feature_matrix_new = fin_group_new.drop(columns= ['Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209', 'Q4210','Q4211', 'Q4212', 'Q4213', 'Q4214', 'Q4215', 'Q4216'])"
      ],
      "metadata": {
        "id": "CoHMWDi4D9dI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valid에 누락된 이미지 응답 결과를 Train 값으로 대체"
      ],
      "metadata": {
        "id": "pNtX9_AhEcow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_df = pd.read_csv('/content/drive/MyDrive/DCC/제출/sim_ori.csv')\n",
        "sim_df.index = sim_df.columns\n",
        "sim_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "p_knEQLMEbcf",
        "outputId": "d74687e7-372d-43e9-eba2-0f9096c454e3",
        "collapsed": true
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          02498     38421     30434     48628     22057     02247     44386  \\\n",
              "02498  1.000001  0.675767  0.785014  0.801968  0.760370  0.785540  0.816377   \n",
              "38421  0.675767  1.000001  0.735742  0.663379  0.732917  0.666624  0.743502   \n",
              "30434  0.785014  0.735742  1.000000  0.815705  0.757281  0.822041  0.811331   \n",
              "\n",
              "          39725     38629     11610     21483     42595  ...     17783  \\\n",
              "02498  0.719492  0.767426  0.735139  0.748646  0.836436  ...  0.856784   \n",
              "38421  0.800364  0.689497  0.736814  0.798023  0.699084  ...  0.602588   \n",
              "30434  0.738502  0.825745  0.746171  0.874997  0.786455  ...  0.790323   \n",
              "\n",
              "          17616     32385     29693     23899     25649     24685     16732  \\\n",
              "02498  0.853685  0.718745  0.849325  0.804929  0.771871  0.688192  0.792840   \n",
              "38421  0.752113  0.797303  0.577520  0.788706  0.638761  0.718256  0.647658   \n",
              "30434  0.734472  0.656305  0.762896  0.721047  0.707614  0.721573  0.725356   \n",
              "\n",
              "          34952     41341     20593     47967  \n",
              "02498  0.882943  0.793323  0.702265  0.789978  \n",
              "38421  0.690893  0.757572  0.834288  0.741483  \n",
              "30434  0.778056  0.836816  0.793110  0.709165  \n",
              "\n",
              "[3 rows x 4486 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5b551c2-f185-4175-b7a9-97d58812e034\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>02498</th>\n",
              "      <th>38421</th>\n",
              "      <th>30434</th>\n",
              "      <th>48628</th>\n",
              "      <th>22057</th>\n",
              "      <th>02247</th>\n",
              "      <th>44386</th>\n",
              "      <th>39725</th>\n",
              "      <th>38629</th>\n",
              "      <th>11610</th>\n",
              "      <th>21483</th>\n",
              "      <th>42595</th>\n",
              "      <th>...</th>\n",
              "      <th>17783</th>\n",
              "      <th>17616</th>\n",
              "      <th>32385</th>\n",
              "      <th>29693</th>\n",
              "      <th>23899</th>\n",
              "      <th>25649</th>\n",
              "      <th>24685</th>\n",
              "      <th>16732</th>\n",
              "      <th>34952</th>\n",
              "      <th>41341</th>\n",
              "      <th>20593</th>\n",
              "      <th>47967</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>02498</th>\n",
              "      <td>1.000001</td>\n",
              "      <td>0.675767</td>\n",
              "      <td>0.785014</td>\n",
              "      <td>0.801968</td>\n",
              "      <td>0.760370</td>\n",
              "      <td>0.785540</td>\n",
              "      <td>0.816377</td>\n",
              "      <td>0.719492</td>\n",
              "      <td>0.767426</td>\n",
              "      <td>0.735139</td>\n",
              "      <td>0.748646</td>\n",
              "      <td>0.836436</td>\n",
              "      <td>...</td>\n",
              "      <td>0.856784</td>\n",
              "      <td>0.853685</td>\n",
              "      <td>0.718745</td>\n",
              "      <td>0.849325</td>\n",
              "      <td>0.804929</td>\n",
              "      <td>0.771871</td>\n",
              "      <td>0.688192</td>\n",
              "      <td>0.792840</td>\n",
              "      <td>0.882943</td>\n",
              "      <td>0.793323</td>\n",
              "      <td>0.702265</td>\n",
              "      <td>0.789978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38421</th>\n",
              "      <td>0.675767</td>\n",
              "      <td>1.000001</td>\n",
              "      <td>0.735742</td>\n",
              "      <td>0.663379</td>\n",
              "      <td>0.732917</td>\n",
              "      <td>0.666624</td>\n",
              "      <td>0.743502</td>\n",
              "      <td>0.800364</td>\n",
              "      <td>0.689497</td>\n",
              "      <td>0.736814</td>\n",
              "      <td>0.798023</td>\n",
              "      <td>0.699084</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602588</td>\n",
              "      <td>0.752113</td>\n",
              "      <td>0.797303</td>\n",
              "      <td>0.577520</td>\n",
              "      <td>0.788706</td>\n",
              "      <td>0.638761</td>\n",
              "      <td>0.718256</td>\n",
              "      <td>0.647658</td>\n",
              "      <td>0.690893</td>\n",
              "      <td>0.757572</td>\n",
              "      <td>0.834288</td>\n",
              "      <td>0.741483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30434</th>\n",
              "      <td>0.785014</td>\n",
              "      <td>0.735742</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.815705</td>\n",
              "      <td>0.757281</td>\n",
              "      <td>0.822041</td>\n",
              "      <td>0.811331</td>\n",
              "      <td>0.738502</td>\n",
              "      <td>0.825745</td>\n",
              "      <td>0.746171</td>\n",
              "      <td>0.874997</td>\n",
              "      <td>0.786455</td>\n",
              "      <td>...</td>\n",
              "      <td>0.790323</td>\n",
              "      <td>0.734472</td>\n",
              "      <td>0.656305</td>\n",
              "      <td>0.762896</td>\n",
              "      <td>0.721047</td>\n",
              "      <td>0.707614</td>\n",
              "      <td>0.721573</td>\n",
              "      <td>0.725356</td>\n",
              "      <td>0.778056</td>\n",
              "      <td>0.836816</td>\n",
              "      <td>0.793110</td>\n",
              "      <td>0.709165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 4486 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5b551c2-f185-4175-b7a9-97d58812e034')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5b551c2-f185-4175-b7a9-97d58812e034 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5b551c2-f185-4175-b7a9-97d58812e034');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e71d2841-0f92-4fa4-b12e-cfc1c64ed31f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e71d2841-0f92-4fa4-b12e-cfc1c64ed31f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e71d2841-0f92-4fa4-b12e-cfc1c64ed31f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sim_df"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valid에만 존재하는 이미지와 가장 유사도가 높은 이미지의 ID를 딕셔너리에 저장"
      ],
      "metadata": {
        "id": "_XfXAX1fVcwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = dict()\n",
        "temp = []\n",
        "\n",
        "for id in inter_valid:\n",
        "  top_list = sim_df[id].sort_values(ascending=False).drop(id).index.tolist()\n",
        "  for top_id in top_list:\n",
        "    if top_id in img_id_list:\n",
        "      temp.append(top_id)\n",
        "      if len(temp) == 1:\n",
        "        result[id] = temp\n",
        "        temp = []\n",
        "        break\n",
        "print(len(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC052hokEpGh",
        "outputId": "58917e25-7b13-4070-95ef-4d2681e7a1c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "응답 결과가 존재하지 않는 Valid의 이미지 응답을 대체"
      ],
      "metadata": {
        "id": "ZRi7DeteViMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, item in result.items():\n",
        "  print(f'Valid Id : {key}')\n",
        "  temp_ = feature_matrix_new.loc[item]\n",
        "  #mode_vector = temp_.mode().iloc[0]\n",
        "  feature_matrix_new.loc[key] = temp_.values.tolist()[0]\n",
        "\n",
        "feature_matrix_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4K16HYZwE1Z-",
        "outputId": "a8f8ab90-ccd3-4a6a-c498-5675bed81331"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Id : 34080\n",
            "Valid Id : 02275\n",
            "Valid Id : 12730\n",
            "Valid Id : 06011\n",
            "Valid Id : 25940\n",
            "Valid Id : 42967\n",
            "Valid Id : 02946\n",
            "Valid Id : 14357\n",
            "Valid Id : 50836\n",
            "Valid Id : 08621\n",
            "Valid Id : 17742\n",
            "Valid Id : 15508\n",
            "Valid Id : 16819\n",
            "Valid Id : 12106\n",
            "Valid Id : 13991\n",
            "Valid Id : 13398\n",
            "Valid Id : 05760\n",
            "Valid Id : 15319\n",
            "Valid Id : 13781\n",
            "Valid Id : 00624\n",
            "Valid Id : 06190\n",
            "Valid Id : 15341\n",
            "Valid Id : 12847\n",
            "Valid Id : 47408\n",
            "Valid Id : 00161\n",
            "Valid Id : 17787\n",
            "Valid Id : 00511\n",
            "Valid Id : 12668\n",
            "Valid Id : 08008\n",
            "Valid Id : 11163\n",
            "Valid Id : 07077\n",
            "Valid Id : 28480\n",
            "Valid Id : 54129\n",
            "Valid Id : 06985\n",
            "Valid Id : 17235\n",
            "Valid Id : 07316\n",
            "Valid Id : 15998\n",
            "Valid Id : 34952\n",
            "Valid Id : 24643\n",
            "Valid Id : 24535\n",
            "Valid Id : 00359\n",
            "Valid Id : 27899\n",
            "Valid Id : 16538\n",
            "Valid Id : 03791\n",
            "Valid Id : 26120\n",
            "Valid Id : 29658\n",
            "Valid Id : 03293\n",
            "Valid Id : 06358\n",
            "Valid Id : 29063\n",
            "Valid Id : 16915\n",
            "Valid Id : 33006\n",
            "Valid Id : 17135\n",
            "Valid Id : 24770\n",
            "Valid Id : 01123\n",
            "Valid Id : 15910\n",
            "Valid Id : 15394\n",
            "Valid Id : 17004\n",
            "Valid Id : 00716\n",
            "Valid Id : 18714\n",
            "Valid Id : 16747\n",
            "Valid Id : 20986\n",
            "Valid Id : 29942\n",
            "Valid Id : 05791\n",
            "Valid Id : 01410\n",
            "Valid Id : 18058\n",
            "Valid Id : 08641\n",
            "Valid Id : 00033\n",
            "Valid Id : 25086\n",
            "Valid Id : 19960\n",
            "Valid Id : 19940\n",
            "Valid Id : 02705\n",
            "Valid Id : 10781\n",
            "Valid Id : 03531\n",
            "Valid Id : 19602\n",
            "Valid Id : 02770\n",
            "Valid Id : 12702\n",
            "Valid Id : 13533\n",
            "Valid Id : 18506\n",
            "Valid Id : 05175\n",
            "Valid Id : 44520\n",
            "Valid Id : 45137\n",
            "Valid Id : 29693\n",
            "Valid Id : 00299\n",
            "Valid Id : 02837\n",
            "Valid Id : 07238\n",
            "Valid Id : 13277\n",
            "Valid Id : 07871\n",
            "Valid Id : 28925\n",
            "Valid Id : 05562\n",
            "Valid Id : 30593\n",
            "Valid Id : 15729\n",
            "Valid Id : 14785\n",
            "Valid Id : 19267\n",
            "Valid Id : 25163\n",
            "Valid Id : 06860\n",
            "Valid Id : 13251\n",
            "Valid Id : 34621\n",
            "Valid Id : 09752\n",
            "Valid Id : 19833\n",
            "Valid Id : 22056\n",
            "Valid Id : 15225\n",
            "Valid Id : 12530\n",
            "Valid Id : 31439\n",
            "Valid Id : 01552\n",
            "Valid Id : 00551\n",
            "Valid Id : 06895\n",
            "Valid Id : 15071\n",
            "Valid Id : 13098\n",
            "Valid Id : 16465\n",
            "Valid Id : 12102\n",
            "Valid Id : 10251\n",
            "Valid Id : 02243\n",
            "Valid Id : 38585\n",
            "Valid Id : 01020\n",
            "Valid Id : 24598\n",
            "Valid Id : 04349\n",
            "Valid Id : 32385\n",
            "Valid Id : 02392\n",
            "Valid Id : 16034\n",
            "Valid Id : 12518\n",
            "Valid Id : 11423\n",
            "Valid Id : 12567\n",
            "Valid Id : 16851\n",
            "Valid Id : 05868\n",
            "Valid Id : 15662\n",
            "Valid Id : 15381\n",
            "Valid Id : 15499\n",
            "Valid Id : 07187\n",
            "Valid Id : 00931\n",
            "Valid Id : 06864\n",
            "Valid Id : 17062\n",
            "Valid Id : 32314\n",
            "Valid Id : 10253\n",
            "Valid Id : 32034\n",
            "Valid Id : 10722\n",
            "Valid Id : 03334\n",
            "Valid Id : 16347\n",
            "Valid Id : 14777\n",
            "Valid Id : 01273\n",
            "Valid Id : 12789\n",
            "Valid Id : 16624\n",
            "Valid Id : 00842\n",
            "Valid Id : 09877\n",
            "Valid Id : 06590\n",
            "Valid Id : 08492\n",
            "Valid Id : 18095\n",
            "Valid Id : 02688\n",
            "Valid Id : 00598\n",
            "Valid Id : 16375\n",
            "Valid Id : 17058\n",
            "Valid Id : 07121\n",
            "Valid Id : 14923\n",
            "Valid Id : 16503\n",
            "Valid Id : 00804\n",
            "Valid Id : 08977\n",
            "Valid Id : 06910\n",
            "Valid Id : 02936\n",
            "Valid Id : 44232\n",
            "Valid Id : 23899\n",
            "Valid Id : 18094\n",
            "Valid Id : 09750\n",
            "Valid Id : 14512\n",
            "Valid Id : 24553\n",
            "Valid Id : 28453\n",
            "Valid Id : 02701\n",
            "Valid Id : 18391\n",
            "Valid Id : 17874\n",
            "Valid Id : 16067\n",
            "Valid Id : 19576\n",
            "Valid Id : 09156\n",
            "Valid Id : 17239\n",
            "Valid Id : 15699\n",
            "Valid Id : 03447\n",
            "Valid Id : 01294\n",
            "Valid Id : 11444\n",
            "Valid Id : 09278\n",
            "Valid Id : 16037\n",
            "Valid Id : 06498\n",
            "Valid Id : 15497\n",
            "Valid Id : 07120\n",
            "Valid Id : 06502\n",
            "Valid Id : 17697\n",
            "Valid Id : 41341\n",
            "Valid Id : 09889\n",
            "Valid Id : 14572\n",
            "Valid Id : 00539\n",
            "Valid Id : 33305\n",
            "Valid Id : 12746\n",
            "Valid Id : 12725\n",
            "Valid Id : 03859\n",
            "Valid Id : 52521\n",
            "Valid Id : 25209\n",
            "Valid Id : 38588\n",
            "Valid Id : 12315\n",
            "Valid Id : 06525\n",
            "Valid Id : 12342\n",
            "Valid Id : 02879\n",
            "Valid Id : 57473\n",
            "Valid Id : 01925\n",
            "Valid Id : 14796\n",
            "Valid Id : 11685\n",
            "Valid Id : 15244\n",
            "Valid Id : 22783\n",
            "Valid Id : 27040\n",
            "Valid Id : 18026\n",
            "Valid Id : 14039\n",
            "Valid Id : 06576\n",
            "Valid Id : 01654\n",
            "Valid Id : 17767\n",
            "Valid Id : 25823\n",
            "Valid Id : 15758\n",
            "Valid Id : 24647\n",
            "Valid Id : 19825\n",
            "Valid Id : 01387\n",
            "Valid Id : 07990\n",
            "Valid Id : 29993\n",
            "Valid Id : 04207\n",
            "Valid Id : 17032\n",
            "Valid Id : 21988\n",
            "Valid Id : 12615\n",
            "Valid Id : 24947\n",
            "Valid Id : 12412\n",
            "Valid Id : 34504\n",
            "Valid Id : 06522\n",
            "Valid Id : 09737\n",
            "Valid Id : 24211\n",
            "Valid Id : 41448\n",
            "Valid Id : 04973\n",
            "Valid Id : 16732\n",
            "Valid Id : 25649\n",
            "Valid Id : 25411\n",
            "Valid Id : 16485\n",
            "Valid Id : 12524\n",
            "Valid Id : 06438\n",
            "Valid Id : 08988\n",
            "Valid Id : 17447\n",
            "Valid Id : 07753\n",
            "Valid Id : 40875\n",
            "Valid Id : 10783\n",
            "Valid Id : 10104\n",
            "Valid Id : 00865\n",
            "Valid Id : 07916\n",
            "Valid Id : 02651\n",
            "Valid Id : 16075\n",
            "Valid Id : 52578\n",
            "Valid Id : 28563\n",
            "Valid Id : 12599\n",
            "Valid Id : 33329\n",
            "Valid Id : 30988\n",
            "Valid Id : 19545\n",
            "Valid Id : 10768\n",
            "Valid Id : 08837\n",
            "Valid Id : 28719\n",
            "Valid Id : 16104\n",
            "Valid Id : 06955\n",
            "Valid Id : 15684\n",
            "Valid Id : 01000\n",
            "Valid Id : 02691\n",
            "Valid Id : 00851\n",
            "Valid Id : 22623\n",
            "Valid Id : 16960\n",
            "Valid Id : 17539\n",
            "Valid Id : 47967\n",
            "Valid Id : 12154\n",
            "Valid Id : 28022\n",
            "Valid Id : 26211\n",
            "Valid Id : 24685\n",
            "Valid Id : 07018\n",
            "Valid Id : 05851\n",
            "Valid Id : 38489\n",
            "Valid Id : 06715\n",
            "Valid Id : 16283\n",
            "Valid Id : 11235\n",
            "Valid Id : 10191\n",
            "Valid Id : 02333\n",
            "Valid Id : 14914\n",
            "Valid Id : 01056\n",
            "Valid Id : 31478\n",
            "Valid Id : 62253\n",
            "Valid Id : 06966\n",
            "Valid Id : 04670\n",
            "Valid Id : 33876\n",
            "Valid Id : 20593\n",
            "Valid Id : 24696\n",
            "Valid Id : 28373\n",
            "Valid Id : 04927\n",
            "Valid Id : 15362\n",
            "Valid Id : 12002\n",
            "Valid Id : 52417\n",
            "Valid Id : 26099\n",
            "Valid Id : 09747\n",
            "Valid Id : 40642\n",
            "Valid Id : 19454\n",
            "Valid Id : 02223\n",
            "Valid Id : 27773\n",
            "Valid Id : 05716\n",
            "Valid Id : 16690\n",
            "Valid Id : 38785\n",
            "Valid Id : 13254\n",
            "Valid Id : 02557\n",
            "Valid Id : 05876\n",
            "Valid Id : 14691\n",
            "Valid Id : 04629\n",
            "Valid Id : 02394\n",
            "Valid Id : 16430\n",
            "Valid Id : 04396\n",
            "Valid Id : 16032\n",
            "Valid Id : 28314\n",
            "Valid Id : 19520\n",
            "Valid Id : 48457\n",
            "Valid Id : 04395\n",
            "Valid Id : 06090\n",
            "Valid Id : 03003\n",
            "Valid Id : 25761\n",
            "Valid Id : 15722\n",
            "Valid Id : 06626\n",
            "Valid Id : 19003\n",
            "Valid Id : 06000\n",
            "Valid Id : 33259\n",
            "Valid Id : 10558\n",
            "Valid Id : 44789\n",
            "Valid Id : 09881\n",
            "Valid Id : 08862\n",
            "Valid Id : 10844\n",
            "Valid Id : 19205\n",
            "Valid Id : 10598\n",
            "Valid Id : 17783\n",
            "Valid Id : 09277\n",
            "Valid Id : 16704\n",
            "Valid Id : 11375\n",
            "Valid Id : 37491\n",
            "Valid Id : 53808\n",
            "Valid Id : 28964\n",
            "Valid Id : 26200\n",
            "Valid Id : 06797\n",
            "Valid Id : 09475\n",
            "Valid Id : 09085\n",
            "Valid Id : 12112\n",
            "Valid Id : 00946\n",
            "Valid Id : 06875\n",
            "Valid Id : 00117\n",
            "Valid Id : 04801\n",
            "Valid Id : 05892\n",
            "Valid Id : 17854\n",
            "Valid Id : 09731\n",
            "Valid Id : 06788\n",
            "Valid Id : 02677\n",
            "Valid Id : 16699\n",
            "Valid Id : 09307\n",
            "Valid Id : 15375\n",
            "Valid Id : 13354\n",
            "Valid Id : 63188\n",
            "Valid Id : 19207\n",
            "Valid Id : 03685\n",
            "Valid Id : 33240\n",
            "Valid Id : 17366\n",
            "Valid Id : 02306\n",
            "Valid Id : 01896\n",
            "Valid Id : 02908\n",
            "Valid Id : 28909\n",
            "Valid Id : 10749\n",
            "Valid Id : 00012\n",
            "Valid Id : 17616\n",
            "Valid Id : 06453\n",
            "Valid Id : 47862\n",
            "Valid Id : 04101\n",
            "Valid Id : 07643\n",
            "Valid Id : 12773\n",
            "Valid Id : 09285\n",
            "Valid Id : 09089\n",
            "Valid Id : 14783\n",
            "Valid Id : 35400\n",
            "Valid Id : 03732\n",
            "Valid Id : 12880\n",
            "Valid Id : 05917\n",
            "Valid Id : 16466\n",
            "Valid Id : 30223\n",
            "Valid Id : 16446\n",
            "Valid Id : 17841\n",
            "Valid Id : 26393\n",
            "Valid Id : 08951\n",
            "Valid Id : 14706\n",
            "Valid Id : 08754\n",
            "Valid Id : 17967\n",
            "Valid Id : 04845\n",
            "Valid Id : 17304\n",
            "Valid Id : 14417\n",
            "Valid Id : 14225\n",
            "Valid Id : 06214\n",
            "Valid Id : 16172\n",
            "Valid Id : 29347\n",
            "Valid Id : 06609\n",
            "Valid Id : 19356\n",
            "Valid Id : 10184\n",
            "Valid Id : 34487\n",
            "Valid Id : 17655\n",
            "Valid Id : 24556\n",
            "Valid Id : 12304\n",
            "Valid Id : 12260\n",
            "Valid Id : 10167\n",
            "Valid Id : 58887\n",
            "Valid Id : 05979\n",
            "Valid Id : 16848\n",
            "Valid Id : 12460\n",
            "Valid Id : 01035\n",
            "Valid Id : 60184\n",
            "Valid Id : 06867\n",
            "Valid Id : 15141\n",
            "Valid Id : 25174\n",
            "Valid Id : 08262\n",
            "Valid Id : 33284\n",
            "Valid Id : 10103\n",
            "Valid Id : 11936\n",
            "Valid Id : 09577\n",
            "Valid Id : 17857\n",
            "Valid Id : 00496\n",
            "Valid Id : 26179\n",
            "Valid Id : 31823\n",
            "Valid Id : 11778\n",
            "Valid Id : 39164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4486, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 이미지 피처 벡터 생성"
      ],
      "metadata": {
        "id": "cBi683nsrMUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_weight_path = \"/content/drive/MyDrive/DCC/제출/segmentation_batch16_lr1e-4_argu_o_schedule_o_labelsmoothing_e0.05.pth\"\n",
        "\n",
        "model = models.resnet18()\n",
        "model.fc = nn.Linear(model.fc.in_features, 31)\n",
        "model.load_state_dict(torch.load(model_weight_path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtzzI4LWqEiF",
        "outputId": "5335f66a-b6b6-47ab-87e8-b25253aa2ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor:\n",
        "  '''\n",
        "  ResNet 모델에서 특정 레이어의 Feature를 추출하기 위한 클래스.\n",
        "\n",
        "  __init__ : avgpool 레이어에 forward hook 등록\n",
        "  hook_fn : avgpool 레이어 출력(feature)을 Flatten 후 저장\n",
        "  close : hook 제거\n",
        "\n",
        "  '''\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.feature = None\n",
        "    self.hook = self.model.avgpool.register_forward_hook(self.hook_fn)\n",
        "\n",
        "  def hook_fn(self, module, input, output):\n",
        "    self.feature = output.view(output.size(0), -1).detach().cpu().numpy()\n",
        "\n",
        "  def close(self):\n",
        "    self.hook.remove()\n",
        "\n",
        "#**모델에 입력하기 위한 이미지 전처리**#\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "gIenBVEMqE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "extractor = FeatureExtractor(model)\n",
        "\n",
        "features_dict = {}\n",
        "unique_image_id = []\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/DCC/2024 데이터 크리에이터 캠프 대학부 데이터셋.zip', 'r') as zip_file:\n",
        "  for file_name in tqdm(zip_file.namelist()):\n",
        "    if not file_name.endswith('.jpg'): continue\n",
        "\n",
        "    #**train 파일과 valid 파일에 존재하는 중복 파일 방지**#\n",
        "    if file_name.startswith('training_image/'):\n",
        "      temp_image_id = file_name.split('_')[2]\n",
        "      if temp_image_id in unique_image_id : continue\n",
        "    else:\n",
        "      temp_image_id = file_name.split('_')[2]\n",
        "      if temp_image_id in unique_image_id : continue\n",
        "\n",
        "    unique_image_id.append(temp_image_id)\n",
        "    #**file_name에 해당하는 이미지를 불러와서 feature vector 추출**#\n",
        "    with zip_file.open(file_name) as file:\n",
        "      image = Image.open(BytesIO(file.read())).convert(\"RGB\")\n",
        "      input_tensor = preprocess(image)\n",
        "      input_batch = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "      #**특징 추출**#\n",
        "      with torch.no_grad():\n",
        "        model(input_batch)\n",
        "        features = extractor.feature\n",
        "        features_dict[temp_image_id] = features\n",
        "\n",
        "#**딕셔너리 형태로 저장된 객체 데이터 프레임으로 변환**#\n",
        "features_dict = {k: v.squeeze() for k, v in features_dict.items()}\n",
        "feature_matrix = pd.DataFrame.from_dict(features_dict, orient='index')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LLHhpbx1qGmp",
        "outputId": "b4f75de1-32ff-4466-8191-37d8e990d626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 252754/252754 [21:53<00:00, 192.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_mat = feature_matrix.copy()"
      ],
      "metadata": {
        "id": "Rtaz-en8T3xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 피처 벡터 불러오기\n",
        "ft_mat = pd.read_csv('/content/drive/MyDrive/DCC/제출/final_feature_matrix.csv')\n",
        "ft_mat.rename(columns={'Unnamed: 0':'img_id'},inplace=True)\n",
        "ft_mat.img_id = ft_mat.img_id.astype(str).str.zfill(5)\n",
        "ft_mat.set_index('img_id',inplace=True)\n",
        "\n",
        "print(ft_mat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V-4mJPmD_6Q",
        "outputId": "be342e16-8542-43e8-c9b1-275563f0ce4e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4486, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 응답 결과 벡터와 이미지 피처 벡터 결합"
      ],
      "metadata": {
        "id": "GWMRaiePvgyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터별 가중치 조절. 이미지 전체의 가중치 합 = 설문 조사 전체 가중치 합\n",
        "ori_pre = ft_mat.copy()\n",
        "ori_pre = ori_pre *(1/512)\n",
        "\n",
        "new_pre = feature_matrix_new.copy()\n",
        "new_pre = new_pre *(1/25)\n",
        "\n",
        "#이어붙인 feature matrix\n",
        "ft_matrix_q = pd.concat([ori_pre,new_pre],axis=1)\n",
        "\n",
        "print(ft_matrix_q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESJphqdWEMWI",
        "outputId": "f36df061-0aac-487f-fd5d-8e5527b112ad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4486, 537)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이어붙인 feature matrix의 유사도 행렬 계산\n",
        "sim_total = pd.DataFrame(cosine_similarity(ft_matrix_q), index=ft_matrix_q.index, columns=ft_matrix_q.index)\n",
        "col = sim_total.columns.astype(str).str.zfill(5)\n",
        "sim_total.columns = col\n",
        "sim_total.index = col\n",
        "sim_total.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "kM93tw-YIEIu",
        "outputId": "6cfba134-3478-4d3a-c592-8e8065280917"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "img_id     02498     38421     30434     48628     22057     02247     44386  \\\n",
              "img_id                                                                         \n",
              "02498   1.000000  0.282606  0.469933  0.575371  0.676416  0.392590  0.587470   \n",
              "38421   0.282606  1.000000  0.567371  0.110635  0.194783  0.463600  0.265155   \n",
              "30434   0.469933  0.567371  1.000000  0.404972  0.387165  0.464697  0.510482   \n",
              "\n",
              "img_id     39725     38629     11610     21483     42595  ...     17783  \\\n",
              "img_id                                                    ...             \n",
              "02498   0.190760  0.425966  0.255293  0.420334  0.217371  ...  0.480149   \n",
              "38421   0.775197  0.205335  0.416331  0.402775  0.593521  ...  0.458427   \n",
              "30434   0.478206  0.411264  0.305772  0.176480  0.689493  ...  0.548476   \n",
              "\n",
              "img_id     17616     32385     29693     23899     25649     24685     16732  \\\n",
              "img_id                                                                         \n",
              "02498   0.310186  0.318138  0.313949  0.625697  0.342130  0.380320  0.292299   \n",
              "38421   0.634097  0.696508  0.105525  0.567690  0.501883  0.714237  0.460414   \n",
              "30434   0.349671  0.547364  0.251865  0.707885  0.392310  0.447650  0.562728   \n",
              "\n",
              "img_id     34952     41341     20593     47967  \n",
              "img_id                                          \n",
              "02498   0.399747  0.467925  0.386510  0.292892  \n",
              "38421   0.560974  0.471290  0.853198  0.718067  \n",
              "30434   0.452779  0.545692  0.689466  0.334738  \n",
              "\n",
              "[3 rows x 4486 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a52c1bf-a83b-4903-ac29-44d06390ce63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>img_id</th>\n",
              "      <th>02498</th>\n",
              "      <th>38421</th>\n",
              "      <th>30434</th>\n",
              "      <th>48628</th>\n",
              "      <th>22057</th>\n",
              "      <th>02247</th>\n",
              "      <th>44386</th>\n",
              "      <th>39725</th>\n",
              "      <th>38629</th>\n",
              "      <th>11610</th>\n",
              "      <th>21483</th>\n",
              "      <th>42595</th>\n",
              "      <th>...</th>\n",
              "      <th>17783</th>\n",
              "      <th>17616</th>\n",
              "      <th>32385</th>\n",
              "      <th>29693</th>\n",
              "      <th>23899</th>\n",
              "      <th>25649</th>\n",
              "      <th>24685</th>\n",
              "      <th>16732</th>\n",
              "      <th>34952</th>\n",
              "      <th>41341</th>\n",
              "      <th>20593</th>\n",
              "      <th>47967</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>img_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>02498</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.282606</td>\n",
              "      <td>0.469933</td>\n",
              "      <td>0.575371</td>\n",
              "      <td>0.676416</td>\n",
              "      <td>0.392590</td>\n",
              "      <td>0.587470</td>\n",
              "      <td>0.190760</td>\n",
              "      <td>0.425966</td>\n",
              "      <td>0.255293</td>\n",
              "      <td>0.420334</td>\n",
              "      <td>0.217371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.480149</td>\n",
              "      <td>0.310186</td>\n",
              "      <td>0.318138</td>\n",
              "      <td>0.313949</td>\n",
              "      <td>0.625697</td>\n",
              "      <td>0.342130</td>\n",
              "      <td>0.380320</td>\n",
              "      <td>0.292299</td>\n",
              "      <td>0.399747</td>\n",
              "      <td>0.467925</td>\n",
              "      <td>0.386510</td>\n",
              "      <td>0.292892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38421</th>\n",
              "      <td>0.282606</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.567371</td>\n",
              "      <td>0.110635</td>\n",
              "      <td>0.194783</td>\n",
              "      <td>0.463600</td>\n",
              "      <td>0.265155</td>\n",
              "      <td>0.775197</td>\n",
              "      <td>0.205335</td>\n",
              "      <td>0.416331</td>\n",
              "      <td>0.402775</td>\n",
              "      <td>0.593521</td>\n",
              "      <td>...</td>\n",
              "      <td>0.458427</td>\n",
              "      <td>0.634097</td>\n",
              "      <td>0.696508</td>\n",
              "      <td>0.105525</td>\n",
              "      <td>0.567690</td>\n",
              "      <td>0.501883</td>\n",
              "      <td>0.714237</td>\n",
              "      <td>0.460414</td>\n",
              "      <td>0.560974</td>\n",
              "      <td>0.471290</td>\n",
              "      <td>0.853198</td>\n",
              "      <td>0.718067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30434</th>\n",
              "      <td>0.469933</td>\n",
              "      <td>0.567371</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.404972</td>\n",
              "      <td>0.387165</td>\n",
              "      <td>0.464697</td>\n",
              "      <td>0.510482</td>\n",
              "      <td>0.478206</td>\n",
              "      <td>0.411264</td>\n",
              "      <td>0.305772</td>\n",
              "      <td>0.176480</td>\n",
              "      <td>0.689493</td>\n",
              "      <td>...</td>\n",
              "      <td>0.548476</td>\n",
              "      <td>0.349671</td>\n",
              "      <td>0.547364</td>\n",
              "      <td>0.251865</td>\n",
              "      <td>0.707885</td>\n",
              "      <td>0.392310</td>\n",
              "      <td>0.447650</td>\n",
              "      <td>0.562728</td>\n",
              "      <td>0.452779</td>\n",
              "      <td>0.545692</td>\n",
              "      <td>0.689466</td>\n",
              "      <td>0.334738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 4486 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a52c1bf-a83b-4903-ac29-44d06390ce63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a52c1bf-a83b-4903-ac29-44d06390ce63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a52c1bf-a83b-4903-ac29-44d06390ce63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5e21834a-99b4-4e80-b9db-8c1c8ec7b2ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e21834a-99b4-4e80-b9db-8c1c8ec7b2ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5e21834a-99b4-4e80-b9db-8c1c8ec7b2ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sim_total"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 성능 확인"
      ],
      "metadata": {
        "id": "vKJwP8orV5q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "저장된 파일을 불러올때"
      ],
      "metadata": {
        "id": "0wigQpECbpqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_matrix = pd.read_csv('/content/drive/MyDrive/DCC/Mission3_upgrade/rating_matrix_Q1Q5_deloutlier.csv')\n",
        "if 'user_id' in rating_matrix.columns:\n",
        "  rating_matrix.set_index('user_id',inplace=True)\n",
        "elif 'id' in rating_matrix.columns:\n",
        "  rating_matrix.set_index('id',inplace=True)\n",
        "\n",
        "sim_df = pd.read_csv('/content/drive/MyDrive/DCC/Mission3_upgrade/sim_df_Novalid.csv')\n",
        "if 'img_id' in sim_df.columns:\n",
        "  sim_df.set_index('img_id',inplace=True)\n",
        "  sim_df.index = sim_df.index.astype(str).str.zfill(5)\n",
        "else:\n",
        "  sim_df.index = sim_df.columns\n",
        "valid_df_t = pd.read_csv('/content/drive/MyDrive/DCC/Mission3_upgrade/valid_rating.csv')\n",
        "valid_df_t.image_id = valid_df_t.image_id.astype(str).str.zfill(5)"
      ],
      "metadata": {
        "id": "NA9bnsrkV7KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파일 불러옴 없이 곧바로 실행할때"
      ],
      "metadata": {
        "id": "7s8DYng4btZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rating matrix\n",
        "if 'user_id' in rating_matrix.columns:\n",
        "  rating_matrix.set_index('user_id',inplace=True)\n",
        "elif 'id' in rating_matrix.columns:\n",
        "  rating_matrix.set_index('id',inplace=True)\n",
        "\n",
        "#valid_df_t\n",
        "valid_df_t.image_id = valid_df_t.image_id.astype(str).str.zfill(5)\n",
        "\n",
        "#similarity matrix\n",
        "sim_df = sim_total.copy()\n",
        "if 'img_id' in sim_df.columns:\n",
        "  sim_df.set_index('img_id',inplace=True)\n",
        "  sim_df.index = sim_df.index.astype(str).str.zfill(5)\n",
        "else:\n",
        "  sim_df.index = sim_df.columns"
      ],
      "metadata": {
        "id": "NanrkVP8cVhT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 통계량 계산 함수"
      ],
      "metadata": {
        "id": "l6DYvfiEV9dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_evaluate(func, rating_matrix, sim_df, valid_df_t, threshold=0, k1=1, min_similarity1=0.5, k2=5, min_similarity2=0.5, min_similarity3=0.5, min_similarity4=0.5):\n",
        "  '''\n",
        "  아이템 기반 협업 필터링 결과를 확인하는 함수.\n",
        "  평가 지표 : 혼동 행렬, 정확도, f1-score\n",
        "\n",
        "  func : 아이템 협업 필터링을 위한 베이스 함수\n",
        "  rating_matrix : 유틸리티 행렬\n",
        "  sim_df : feature 유사도 행렬\n",
        "  valid_df_t : 검증 데이터\n",
        "  threshold : 선호도 판단 기준\n",
        "  k : 유사도 선택 개수\n",
        "  '''\n",
        "\n",
        "  # 모든 예측 수행\n",
        "  if func.__name__ == 'predict':\n",
        "    pred_list = [func(valid_df_t.iloc[i, 0], str(valid_df_t.iloc[i, 2]), threshold = threshold) for i in range(len(valid_df_t))]\n",
        "\n",
        "  elif func.__name__ == 'predict_v2_1':#2번 변형\n",
        "    pred_list = [func(valid_df_t.iloc[i, 0], str(valid_df_t.iloc[i, 2]), threshold = threshold, k1 = k1, min_similarity1 = min_similarity1, min_similarity2 = min_similarity2, k2 = k2) for i in range(len(valid_df_t))]\n",
        "\n",
        "\n",
        "  # 예측 결과 저장\n",
        "  valid_df_t['predict'] = pred_list\n",
        "\n",
        "  # 평가 지표 계산\n",
        "  conf_matrix = confusion_matrix(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  acc = accuracy_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  rcs = recall_score(valid_df_t['rating'], valid_df_t['predict'],average='binary')\n",
        "  pcs = precision_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  f1 = f1_score(valid_df_t['rating'], valid_df_t['predict'], average='binary')\n",
        "\n",
        "  # # 결과 출력 - 하이퍼 파라미터 튜닝 중에는 잠시 출력 정지\n",
        "  print(\"Confusion Matrix:\\n\", pd.DataFrame(conf_matrix, index=['N', 'P'], columns=['N', 'P']))\n",
        "  print(f\"Accuracy: {acc:.4f}\")\n",
        "  # print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "  return acc, rcs, pcs, f1"
      ],
      "metadata": {
        "id": "PY7yQ7K6V9lL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1번 Base 함수"
      ],
      "metadata": {
        "id": "Azy-kNBYWCJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(user_id, item_id, threshold):\n",
        "  '''\n",
        "  feature 유사도 행렬을 이용해서 아이템 기반 협업 필터링을 수행하는 기본적인 함수.\n",
        "\n",
        "  user_id : 사용자 식별 ID\n",
        "  item_id : 아이템 식별 ID\n",
        "  threshold : 선호도 판단 기준\n",
        "\n",
        "  rating_matrix : 유틸리티 행렬\n",
        "  sim_df : feature 유사도 행렬\n",
        "  '''\n",
        "  # 유저가 평가한 아이템의 선호도 1 또는 0 또는 Nan\n",
        "\n",
        "  user_ratings = rating_matrix.loc[user_id].dropna()\n",
        "\n",
        "  # 해당 아이템과 평가된 아이템 간 유사도\n",
        "  similarities = sim_df[item_id].loc[user_ratings.index]\n",
        "\n",
        "  if item_id in similarities.index: similarities.drop(index=item_id)\n",
        "\n",
        "  valid_indices = similarities.index\n",
        "\n",
        "  # 유사도와 평점의 내적 계산\n",
        "  prediction = np.dot(similarities, user_ratings) / similarities.sum()\n",
        "\n",
        "  # 임계값 기준으로 이진 분류 (1 또는 0)\n",
        "  return 1 if prediction >= threshold else -1\n",
        "\n",
        "predict_and_evaluate(predict, rating_matrix, sim_df, valid_df_t, threshold = 0, k1=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkFcIb_8WCON",
        "outputId": "e8735c97-4434-45a5-96b8-79650779618e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  561   99\n",
            "P  248  194\n",
            "Accuracy: 0.6851\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6851179673321234,\n",
              " 0.43891402714932126,\n",
              " 0.6621160409556314,\n",
              " 0.527891156462585)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2번 함수"
      ],
      "metadata": {
        "id": "PtjCPs5GWCSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_v2_1(user_id, item_id, threshold, k1, min_similarity1, min_similarity2, k2):\n",
        "  '''\n",
        "  위에 버전에서 min_similarity = 0.92에 대해 k1 = 1 인 아이템 찾음.\n",
        "  나머지 아이템 들에 대해서는 min_similarity2, k2 = k2를 적용함.\n",
        "\n",
        "  user_id : 사용자 식별 ID\n",
        "  item_id : 아이템 식별 ID\n",
        "  threshold : 선호도 판단 기준\n",
        "  k1: 높은 유사도 선택 개수\n",
        "  k2 : 유사도 선택 개수\n",
        "  min_similarity1 : 최소 유사도 값\n",
        "  min_similarity2: min_similarity를 만족시키지 못한 아이템들의 차선 similarity\n",
        "  rating_matrix : 유틸리티 행렬\n",
        "  sim_df : feature 유사도 행렬\n",
        "      '''\n",
        "  user_ratings = rating_matrix.loc[user_id].dropna()\n",
        "  similarities = sim_df[item_id].loc[user_ratings.index]\n",
        "\n",
        "  valid_indices_pri = similarities[similarities >= min_similarity1].nlargest(k1).index\n",
        "  valid_indices = similarities[similarities >= min_similarity2].nlargest(k2).index\n",
        "\n",
        "  #유사도가 0.92 이상인게 있으면,k1 = 1 사용\n",
        "  if len(valid_indices_pri) > 0:\n",
        "    # 유사도 0.92 이상을 만족하는 상위 1개 아이템의 유사도와 평점을 사용, 정확도 매우 좋음\n",
        "    filtered_similarities = similarities[valid_indices_pri]\n",
        "    filtered_ratings = user_ratings[valid_indices_pri]\n",
        "\n",
        "\n",
        "  #유사도가 0.92이상인게 없을 때,min_similarity2 기준으로 상위 k개 선택\n",
        "  elif len(valid_indices)>0:\n",
        "\n",
        "    filtered_similarities = similarities[valid_indices]\n",
        "    filtered_ratings = user_ratings[valid_indices]\n",
        "    weighted_sum = np.dot(filtered_similarities, filtered_ratings)\n",
        "    prediction = weighted_sum / filtered_similarities.sum()\n",
        "\n",
        "  else:\n",
        "    # 유사도가 충분하지 않아 예측 불가\n",
        "    filtered_ratings = user_ratings.values\n",
        "    filtered_similarities = similarities\n",
        "\n",
        "\n",
        "  weighted_sum = np.dot(filtered_similarities, filtered_ratings)\n",
        "  prediction = weighted_sum / filtered_similarities.sum()\n",
        "\n",
        "  # 임계값 기준으로 이진 분류 (-1 또는 1)\n",
        "\n",
        "  return 1 if prediction >= threshold else -1\n",
        "\n",
        "predict_and_evaluate(predict_v2_1, rating_matrix, sim_df, valid_df_t, threshold= 0, k1=1, min_similarity1 = 0.92, min_similarity2 = 0.41, k2 = 34)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgtW-L7ZWCXb",
        "outputId": "1e3b7b72-4003-4464-ea34-48fec25b5a37"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  602   58\n",
            "P  110  332\n",
            "Accuracy: 0.8475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8475499092558983, 0.751131221719457, 0.8512820512820513, 0.7980769230769231)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Optuna 최적화"
      ],
      "metadata": {
        "id": "_DA3jXPJWf5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39m1fjxWiZN",
        "outputId": "348f4162-1632-4cfc-efe2-0a9b5d9a6d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "NLJXaxKuWmLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_predict_v1(trial):\n",
        "  '''\n",
        "  주어진 함수에서 최적의 하이퍼 파라미터를 탐색하는 함수\n",
        "  최적화 기준은 Accuracy로 진행. 정의된 f1 score, recall, precision과 같은 지표로도 최적화 수행 가능\n",
        "  '''\n",
        "  # 하이퍼파라미터 탐색 공간 정의\n",
        "  threshold = trial.suggest_float(\"threshold\", -0.5, 0.5, step=0.1)\n",
        "  # 하이퍼파라미터 튜닝\n",
        "  valid_df_t['predict'] = valid_df_t.apply(lambda x: predict(x[0], str(x[2]), threshold), axis=1)\n",
        "  f1 = f1_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  acc = accuracy_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  rcs = recall_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  pcs = precision_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "\n",
        "  return acc\n",
        "\n",
        "def objective_predict_v2_1(trial):\n",
        "  threshold = trial.suggest_float(\"threshold\", -0.5, 0.5, step=0.1)\n",
        "  k = trial.suggest_int(\"k\", 1,1)\n",
        "  min_similarity1 = trial.suggest_float(\"min_similarity1\", 0.7, 0.99, step=0.01)\n",
        "  min_similarity2 = trial.suggest_float(\"min_similarity2\", 0.2, 0.6, step=0.01)\n",
        "  k2 = trial.suggest_int(\"k2\", 30, 50)\n",
        "  valid_df_t['predict'] = valid_df_t.apply(lambda x: predict_v2_1(x[0], str(x[2]),threshold,k,min_similarity1,min_similarity2, k2), axis=1)\n",
        "\n",
        "  f1 = f1_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  acc = accuracy_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  rcs = recall_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  pcs = precision_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "rsvkedJ7WqV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna 튜닝 실행\n",
        "predict_and_evaluate(predict, rating_matrix, sim_df, valid_df_t)\n",
        "\n",
        "study_v1 = optuna.create_study(direction=\"maximize\")\n",
        "study_v1.optimize(objective_predict_v1, n_trials=30)\n",
        "\n",
        "print(\"Best parameters:\", study_v1.best_params)\n",
        "print(\"Best Accuracy:\", study_v1.best_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bIYta4LW1XE",
        "outputId": "ab4933f6-9382-48a3-e34e-17e74c5f3b3c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-26 14:22:34,352] A new study created in memory with name: no-name-e8505801-adc0-4324-b293-c0ede6c996fe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  561   99\n",
            "P  248  194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-26 14:22:36,530] Trial 0 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:39,094] Trial 1 finished with value: 0.573502722323049 and parameters: {'threshold': -0.4}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:41,472] Trial 2 finished with value: 0.6851179673321234 and parameters: {'threshold': 0.0}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:45,966] Trial 3 finished with value: 0.6188747731397459 and parameters: {'threshold': 0.30000000000000004}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:48,811] Trial 4 finished with value: 0.6651542649727767 and parameters: {'threshold': 0.10000000000000009}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:51,097] Trial 5 finished with value: 0.6851179673321234 and parameters: {'threshold': 0.0}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:52,863] Trial 6 finished with value: 0.6651542649727767 and parameters: {'threshold': 0.10000000000000009}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:54,811] Trial 7 finished with value: 0.6188747731397459 and parameters: {'threshold': 0.30000000000000004}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:56,110] Trial 8 finished with value: 0.6660617059891107 and parameters: {'threshold': -0.19999999999999996}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:57,354] Trial 9 finished with value: 0.6506352087114338 and parameters: {'threshold': 0.20000000000000007}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:22:58,707] Trial 10 finished with value: 0.6079854809437386 and parameters: {'threshold': 0.5}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:00,121] Trial 11 finished with value: 0.6660617059891107 and parameters: {'threshold': -0.19999999999999996}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:01,374] Trial 12 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:02,375] Trial 13 finished with value: 0.6660617059891107 and parameters: {'threshold': -0.19999999999999996}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:03,363] Trial 14 finished with value: 0.5063520871143375 and parameters: {'threshold': -0.5}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:04,347] Trial 15 finished with value: 0.6388384754990926 and parameters: {'threshold': -0.3}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:05,347] Trial 16 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:06,337] Trial 17 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:07,311] Trial 18 finished with value: 0.6388384754990926 and parameters: {'threshold': -0.3}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:08,325] Trial 19 finished with value: 0.6079854809437386 and parameters: {'threshold': 0.5}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:09,316] Trial 20 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:10,312] Trial 21 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:11,442] Trial 22 finished with value: 0.6651542649727767 and parameters: {'threshold': 0.10000000000000009}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:12,805] Trial 23 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:14,236] Trial 24 finished with value: 0.6388384754990926 and parameters: {'threshold': -0.3}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:15,682] Trial 25 finished with value: 0.6851179673321234 and parameters: {'threshold': 0.0}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:16,682] Trial 26 finished with value: 0.573502722323049 and parameters: {'threshold': -0.4}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:17,672] Trial 27 finished with value: 0.6851179673321234 and parameters: {'threshold': 0.0}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:18,672] Trial 28 finished with value: 0.6923774954627949 and parameters: {'threshold': -0.09999999999999998}. Best is trial 0 with value: 0.6923774954627949.\n",
            "[I 2024-11-26 14:23:19,675] Trial 29 finished with value: 0.573502722323049 and parameters: {'threshold': -0.4}. Best is trial 0 with value: 0.6923774954627949.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'threshold': -0.09999999999999998}\n",
            "Best Accuracy: 0.6923774954627949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(predict_v2_1, rating_matrix, sim_df, valid_df_t, threshold= -0.1, k1=1, min_similarity1 = 0.92, min_similarity2 = 0.6, k2 = 37)\n",
        "\n",
        "study_v2 = optuna.create_study(direction=\"maximize\")\n",
        "study_v2.optimize(objective_predict_v2_1, n_trials=300)\n",
        "\n",
        "print(\"Best parameters:\", study_v2.best_params)\n",
        "print(\"Best Accuracy:\", study_v2.best_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYEhOTHfWzgU",
        "outputId": "2d8be509-24c9-452b-b5e0-a0b5a14eb123",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-26 14:55:33,523] A new study created in memory with name: no-name-68c8539f-fade-43ba-b84e-bbfc96b53c5c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  574   86\n",
            "P   85  357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-26 14:55:38,408] Trial 0 finished with value: 0.809437386569873 and parameters: {'threshold': -0.5, 'k': 1, 'min_similarity1': 0.83, 'min_similarity2': 0.4, 'k2': 47}. Best is trial 0 with value: 0.809437386569873.\n",
            "[I 2024-11-26 14:55:43,567] Trial 1 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.82, 'min_similarity2': 0.35, 'k2': 31}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:55:47,833] Trial 2 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.87, 'min_similarity2': 0.39, 'k2': 42}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:55:50,807] Trial 3 finished with value: 0.838475499092559 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.8899999999999999, 'min_similarity2': 0.46, 'k2': 46}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:55:54,037] Trial 4 finished with value: 0.8411978221415608 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.8799999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 42}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:55:59,930] Trial 5 finished with value: 0.8402903811252269 and parameters: {'threshold': -0.3, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.26, 'k2': 37}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:56:05,759] Trial 6 finished with value: 0.8130671506352087 and parameters: {'threshold': -0.5, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.44, 'k2': 42}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:56:12,919] Trial 7 finished with value: 0.8139745916515426 and parameters: {'threshold': -0.4, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.49, 'k2': 46}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:56:19,452] Trial 8 finished with value: 0.838475499092559 and parameters: {'threshold': 0.30000000000000004, 'k': 1, 'min_similarity1': 0.8799999999999999, 'min_similarity2': 0.5900000000000001, 'k2': 42}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:56:23,965] Trial 9 finished with value: 0.8366606170598911 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.86, 'min_similarity2': 0.21000000000000002, 'k2': 31}. Best is trial 1 with value: 0.8439201451905626.\n",
            "[I 2024-11-26 14:56:28,426] Trial 10 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.35, 'k2': 30}. Best is trial 10 with value: 0.8484573502722323.\n",
            "[I 2024-11-26 14:56:31,713] Trial 11 finished with value: 0.8348457350272233 and parameters: {'threshold': 0.5, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.32, 'k2': 30}. Best is trial 10 with value: 0.8484573502722323.\n",
            "[I 2024-11-26 14:56:34,911] Trial 12 finished with value: 0.8475499092558983 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7899999999999999, 'min_similarity2': 0.35, 'k2': 34}. Best is trial 10 with value: 0.8484573502722323.\n",
            "[I 2024-11-26 14:56:40,204] Trial 13 finished with value: 0.8430127041742287 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7799999999999999, 'min_similarity2': 0.54, 'k2': 35}. Best is trial 10 with value: 0.8484573502722323.\n",
            "[I 2024-11-26 14:56:45,206] Trial 14 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.7899999999999999, 'min_similarity2': 0.36, 'k2': 35}. Best is trial 10 with value: 0.8484573502722323.\n",
            "[I 2024-11-26 14:56:48,456] Trial 15 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.27, 'k2': 34}. Best is trial 10 with value: 0.8484573502722323.\n",
            "[I 2024-11-26 14:56:51,769] Trial 16 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.21000000000000002, 'k2': 38}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:56:55,389] Trial 17 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:56:59,390] Trial 18 finished with value: 0.8348457350272233 and parameters: {'threshold': 0.4, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.21000000000000002, 'k2': 38}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:02,549] Trial 19 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.26, 'k2': 39}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:06,038] Trial 20 finished with value: 0.837568058076225 and parameters: {'threshold': -0.3, 'k': 1, 'min_similarity1': 0.99, 'min_similarity2': 0.30000000000000004, 'k2': 32}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:09,487] Trial 21 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.2, 'k2': 49}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:13,395] Trial 22 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:16,511] Trial 23 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 44}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:19,621] Trial 24 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.77, 'min_similarity2': 0.30000000000000004, 'k2': 37}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:22,922] Trial 25 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 40}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:27,004] Trial 26 finished with value: 0.8411978221415608 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.8099999999999999, 'min_similarity2': 0.2, 'k2': 44}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:30,073] Trial 27 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.44, 'k2': 33}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:33,148] Trial 28 finished with value: 0.8357531760435571 and parameters: {'threshold': -0.3, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.28, 'k2': 50}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:36,117] Trial 29 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.83, 'min_similarity2': 0.39, 'k2': 44}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:40,331] Trial 30 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.30000000000000004, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.33, 'k2': 36}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:43,649] Trial 31 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.2, 'k2': 49}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:46,730] Trial 32 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.23, 'k2': 50}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:49,790] Trial 33 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 48}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:53,549] Trial 34 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.2, 'k2': 47}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:57:57,159] Trial 35 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.8099999999999999, 'min_similarity2': 0.28, 'k2': 46}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:00,207] Trial 36 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.25, 'k2': 48}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:03,244] Trial 37 finished with value: 0.8366606170598911 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.51, 'k2': 40}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:07,397] Trial 38 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.22, 'k2': 45}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:13,169] Trial 39 finished with value: 0.8421052631578947 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.77, 'min_similarity2': 0.42000000000000004, 'k2': 45}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:16,840] Trial 40 finished with value: 0.8430127041742287 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.84, 'min_similarity2': 0.36, 'k2': 42}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:19,979] Trial 41 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.22, 'k2': 48}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:24,326] Trial 42 finished with value: 0.8430127041742287 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.26, 'k2': 50}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:27,399] Trial 43 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.22, 'k2': 47}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:30,538] Trial 44 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.30000000000000004, 'k2': 41}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:33,611] Trial 45 finished with value: 0.8303085299455535 and parameters: {'threshold': -0.4, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.25, 'k2': 43}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:37,316] Trial 46 finished with value: 0.8421052631578947 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.33, 'k2': 46}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:40,808] Trial 47 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.39, 'k2': 30}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:43,850] Trial 48 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.28, 'k2': 49}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:48,965] Trial 49 finished with value: 0.837568058076225 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7899999999999999, 'min_similarity2': 0.5700000000000001, 'k2': 47}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:58:57,181] Trial 50 finished with value: 0.8457350272232305 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.47000000000000003, 'k2': 32}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:06,164] Trial 51 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.22, 'k2': 45}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:09,992] Trial 52 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.22, 'k2': 43}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:13,013] Trial 53 finished with value: 0.8475499092558983 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7799999999999999, 'min_similarity2': 0.22, 'k2': 43}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:16,033] Trial 54 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.25, 'k2': 45}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:19,418] Trial 55 finished with value: 0.8430127041742287 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.7999999999999999, 'min_similarity2': 0.21000000000000002, 'k2': 39}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:23,520] Trial 56 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.86, 'min_similarity2': 0.27, 'k2': 41}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:26,608] Trial 57 finished with value: 0.8366606170598911 and parameters: {'threshold': 0.30000000000000004, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.24000000000000002, 'k2': 43}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:30,883] Trial 58 finished with value: 0.8466424682395645 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.77, 'min_similarity2': 0.22, 'k2': 38}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:34,249] Trial 59 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.2, 'k2': 45}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:38,104] Trial 60 finished with value: 0.8366606170598911 and parameters: {'threshold': -0.3, 'k': 1, 'min_similarity1': 0.9099999999999999, 'min_similarity2': 0.29000000000000004, 'k2': 41}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:41,180] Trial 61 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.2, 'k2': 45}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:44,254] Trial 62 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.23, 'k2': 44}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:47,316] Trial 63 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.21000000000000002, 'k2': 49}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:51,667] Trial 64 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.7799999999999999, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 14:59:58,298] Trial 65 finished with value: 0.8475499092558983 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.76, 'min_similarity2': 0.27, 'k2': 31}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:03,857] Trial 66 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.21000000000000002, 'k2': 46}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:09,204] Trial 67 finished with value: 0.8430127041742287 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.26, 'k2': 46}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:12,729] Trial 68 finished with value: 0.8475499092558983 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.37, 'k2': 43}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:15,873] Trial 69 finished with value: 0.8475499092558983 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.32, 'k2': 35}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:20,010] Trial 70 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.22, 'k2': 46}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:23,393] Trial 71 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.21000000000000002, 'k2': 48}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:26,487] Trial 72 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.21000000000000002, 'k2': 48}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:29,586] Trial 73 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.21000000000000002, 'k2': 48}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:33,393] Trial 74 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 16 with value: 0.8502722323049002.\n",
            "[I 2024-11-26 15:00:37,101] Trial 75 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:00:40,188] Trial 76 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:00:43,259] Trial 77 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:00:50,034] Trial 78 finished with value: 0.8430127041742287 and parameters: {'threshold': 0.5, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.25, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:00:55,106] Trial 79 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.23, 'k2': 37}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:00:58,181] Trial 80 finished with value: 0.8457350272232305 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.26, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:01,691] Trial 81 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.2, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:08,848] Trial 82 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.22, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:12,354] Trial 83 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.21000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:16,925] Trial 84 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 44}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:26,550] Trial 85 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.21000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:29,550] Trial 86 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:33,278] Trial 87 finished with value: 0.8421052631578947 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.25, 'k2': 45}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:36,974] Trial 88 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.21000000000000002, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:39,813] Trial 89 finished with value: 0.8493647912885662 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.98, 'min_similarity2': 0.27, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:44,065] Trial 90 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.20000000000000007, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.22, 'k2': 39}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:49,372] Trial 91 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:53,389] Trial 92 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:01:56,523] Trial 93 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:00,987] Trial 94 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:05,490] Trial 95 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:09,285] Trial 96 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.24000000000000002, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:12,409] Trial 97 finished with value: 0.8475499092558983 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.21000000000000002, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:19,601] Trial 98 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.22, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:27,024] Trial 99 finished with value: 0.8130671506352087 and parameters: {'threshold': -0.5, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:32,413] Trial 100 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:36,104] Trial 101 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:39,177] Trial 102 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.25, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:42,253] Trial 103 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:45,808] Trial 104 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:49,643] Trial 105 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.26, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:53,245] Trial 106 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.23, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:56,343] Trial 107 finished with value: 0.8466424682395645 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.25, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:02:59,858] Trial 108 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.28, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:03,959] Trial 109 finished with value: 0.8411978221415608 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.22, 'k2': 38}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:07,006] Trial 110 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:10,051] Trial 111 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.21000000000000002, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:13,235] Trial 112 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:17,790] Trial 113 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.21000000000000002, 'k2': 45}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:24,309] Trial 114 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:31,153] Trial 115 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.22, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:35,691] Trial 116 finished with value: 0.8457350272232305 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:38,840] Trial 117 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.26, 'k2': 37}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:41,768] Trial 118 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.82, 'min_similarity2': 0.52, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:45,831] Trial 119 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:49,247] Trial 120 finished with value: 0.8466424682395645 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.21000000000000002, 'k2': 42}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:52,281] Trial 121 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:55,314] Trial 122 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.42000000000000004, 'k2': 40}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:03:59,048] Trial 123 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.22, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:02,774] Trial 124 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.2, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:05,831] Trial 125 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.22, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:08,950] Trial 126 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.21000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:12,859] Trial 127 finished with value: 0.838475499092559 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:16,744] Trial 128 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.75, 'min_similarity2': 0.2, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:19,815] Trial 129 finished with value: 0.8457350272232305 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.25, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:22,834] Trial 130 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.21000000000000002, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:26,228] Trial 131 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 36}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:30,475] Trial 132 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.22, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:33,554] Trial 133 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.2, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:36,468] Trial 134 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.8799999999999999, 'min_similarity2': 0.21000000000000002, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:39,512] Trial 135 finished with value: 0.8457350272232305 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:43,396] Trial 136 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.22, 'k2': 50}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:46,934] Trial 137 finished with value: 0.8402903811252269 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:50,014] Trial 138 finished with value: 0.8493647912885662 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.2, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:53,090] Trial 139 finished with value: 0.8457350272232305 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.21000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:04:56,719] Trial 140 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:00,530] Trial 141 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:03,626] Trial 142 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:06,700] Trial 143 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:10,032] Trial 144 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.25, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:14,118] Trial 145 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.27, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:17,174] Trial 146 finished with value: 0.8439201451905626 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:20,281] Trial 147 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:23,377] Trial 148 finished with value: 0.8475499092558983 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.26, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:27,608] Trial 149 finished with value: 0.8339382940108893 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.6, 'k2': 45}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:30,833] Trial 150 finished with value: 0.8475499092558983 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:33,761] Trial 151 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.85, 'min_similarity2': 0.22, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:36,819] Trial 152 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.23, 'k2': 49}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:40,628] Trial 153 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.22, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:44,343] Trial 154 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.25, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:47,415] Trial 155 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.22, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:50,551] Trial 156 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:54,077] Trial 157 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:05:58,663] Trial 158 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.25, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:01,754] Trial 159 finished with value: 0.838475499092559 and parameters: {'threshold': -0.4, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:04,839] Trial 160 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.7, 'min_similarity2': 0.22, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:08,360] Trial 161 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:12,322] Trial 162 finished with value: 0.8466424682395645 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:15,424] Trial 163 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.23, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:18,535] Trial 164 finished with value: 0.8475499092558983 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.26, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:21,764] Trial 165 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.25, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:25,858] Trial 166 finished with value: 0.8321234119782214 and parameters: {'threshold': 0.4, 'k': 1, 'min_similarity1': 0.9099999999999999, 'min_similarity2': 0.23, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:28,996] Trial 167 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:32,217] Trial 168 finished with value: 0.8430127041742287 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.74, 'min_similarity2': 0.22, 'k2': 39}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:35,361] Trial 169 finished with value: 0.8448275862068966 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.23, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:39,443] Trial 170 finished with value: 0.8421052631578947 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.48000000000000004, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:42,721] Trial 171 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.22, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:45,795] Trial 172 finished with value: 0.8502722323049002 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.71, 'min_similarity2': 0.22, 'k2': 48}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:48,865] Trial 173 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:52,631] Trial 174 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:56,390] Trial 175 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:06:59,533] Trial 176 finished with value: 0.8421052631578947 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.27, 'k2': 46}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:07:02,593] Trial 177 finished with value: 0.8484573502722323 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.72, 'min_similarity2': 0.25, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:07:06,145] Trial 178 finished with value: 0.8430127041742287 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.73, 'min_similarity2': 0.26, 'k2': 47}. Best is trial 75 with value: 0.8511796733212341.\n",
            "[I 2024-11-26 15:07:09,892] Trial 179 finished with value: 0.8539019963702359 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:12,761] Trial 180 finished with value: 0.8393829401088929 and parameters: {'threshold': 0.10000000000000009, 'k': 1, 'min_similarity1': 0.96, 'min_similarity2': 0.24000000000000002, 'k2': 48}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:15,624] Trial 181 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.96, 'min_similarity2': 0.25, 'k2': 46}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:18,575] Trial 182 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.96, 'min_similarity2': 0.25, 'k2': 46}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:22,497] Trial 183 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.25, 'k2': 46}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:25,620] Trial 184 finished with value: 0.8511796733212341 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.96, 'min_similarity2': 0.25, 'k2': 47}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:28,506] Trial 185 finished with value: 0.852994555353902 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.98, 'min_similarity2': 0.24000000000000002, 'k2': 46}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:31,435] Trial 186 finished with value: 0.8539019963702359 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.24000000000000002, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:34,764] Trial 187 finished with value: 0.852994555353902 and parameters: {'threshold': 0.0, 'k': 1, 'min_similarity1': 0.98, 'min_similarity2': 0.24000000000000002, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:38,506] Trial 188 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:41,379] Trial 189 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.99, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:44,269] Trial 190 finished with value: 0.838475499092559 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.99, 'min_similarity2': 0.23, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:47,144] Trial 191 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.98, 'min_similarity2': 0.24000000000000002, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:51,018] Trial 192 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:54,284] Trial 193 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:07:57,183] Trial 194 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:00,132] Trial 195 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.23, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:03,450] Trial 196 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.26, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:07,275] Trial 197 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.26, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:10,160] Trial 198 finished with value: 0.8393829401088929 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.5700000000000001, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:13,084] Trial 199 finished with value: 0.8421052631578947 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.23, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:15,942] Trial 200 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.26, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:19,722] Trial 201 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.26, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:23,039] Trial 202 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.27, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:25,906] Trial 203 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.27, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:28,772] Trial 204 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.28, 'k2': 45}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:31,883] Trial 205 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:35,798] Trial 206 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:38,662] Trial 207 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:41,548] Trial 208 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 179 with value: 0.8539019963702359.\n",
            "[I 2024-11-26 15:08:44,422] Trial 209 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:08:48,027] Trial 210 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:08:51,499] Trial 211 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.30000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:08:54,374] Trial 212 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:08:57,263] Trial 213 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:00,317] Trial 214 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.29000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:04,386] Trial 215 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.32, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:07,269] Trial 216 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.32, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:10,143] Trial 217 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:12,988] Trial 218 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:16,528] Trial 219 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:20,088] Trial 220 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.32, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:22,962] Trial 221 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:25,815] Trial 222 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:28,653] Trial 223 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.32, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:32,674] Trial 224 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:35,770] Trial 225 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:38,642] Trial 226 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:41,533] Trial 227 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:44,805] Trial 228 finished with value: 0.8430127041742287 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.98, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:48,538] Trial 229 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:51,407] Trial 230 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.33, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:54,278] Trial 231 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:09:57,168] Trial 232 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:00,937] Trial 233 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:04,331] Trial 234 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:07,208] Trial 235 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:10,096] Trial 236 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9099999999999999, 'min_similarity2': 0.33, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:13,146] Trial 237 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:17,107] Trial 238 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:19,995] Trial 239 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.97, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:22,850] Trial 240 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:25,721] Trial 241 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.97, 'min_similarity2': 0.34, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:29,329] Trial 242 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:32,812] Trial 243 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:35,642] Trial 244 finished with value: 0.8466424682395645 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:38,529] Trial 245 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:41,479] Trial 246 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9099999999999999, 'min_similarity2': 0.33, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:45,543] Trial 247 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:48,489] Trial 248 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:51,390] Trial 249 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:54,241] Trial 250 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:10:57,664] Trial 251 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:01,263] Trial 252 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:04,125] Trial 253 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.34, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:07,018] Trial 254 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:09,893] Trial 255 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.34, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:13,918] Trial 256 finished with value: 0.8448275862068966 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:16,913] Trial 257 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.33, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:19,811] Trial 258 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:22,684] Trial 259 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9099999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:26,071] Trial 260 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:29,753] Trial 261 finished with value: 0.8430127041742287 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.33, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:32,669] Trial 262 finished with value: 0.8475499092558983 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.8999999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:35,547] Trial 263 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 42}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:38,417] Trial 264 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.32, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:42,342] Trial 265 finished with value: 0.8539019963702359 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.32, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:45,516] Trial 266 finished with value: 0.8493647912885662 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.36, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:48,398] Trial 267 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.33, 'k2': 42}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:51,264] Trial 268 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.32, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:54,461] Trial 269 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:11:58,281] Trial 270 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 42}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:01,226] Trial 271 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 41}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:04,125] Trial 272 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:07,003] Trial 273 finished with value: 0.8493647912885662 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 42}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:10,875] Trial 274 finished with value: 0.8511796733212341 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.34, 'k2': 42}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:14,112] Trial 275 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:17,050] Trial 276 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:19,971] Trial 277 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:23,215] Trial 278 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.30000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:27,139] Trial 279 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:30,015] Trial 280 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:32,986] Trial 281 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.30000000000000004, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:35,905] Trial 282 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:39,679] Trial 283 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.38, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:42,972] Trial 284 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:45,862] Trial 285 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.31, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:48,734] Trial 286 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:51,868] Trial 287 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.30000000000000004, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:55,823] Trial 288 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:12:58,760] Trial 289 finished with value: 0.852994555353902 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.31, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:01,631] Trial 290 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.30000000000000004, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:04,545] Trial 291 finished with value: 0.8484573502722323 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:08,142] Trial 292 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:11,545] Trial 293 finished with value: 0.8502722323049002 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.41000000000000003, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:14,459] Trial 294 finished with value: 0.8466424682395645 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.95, 'min_similarity2': 0.44, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:17,352] Trial 295 finished with value: 0.8439201451905626 and parameters: {'threshold': -0.19999999999999996, 'k': 1, 'min_similarity1': 0.9099999999999999, 'min_similarity2': 0.29000000000000004, 'k2': 46}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:20,522] Trial 296 finished with value: 0.8493647912885662 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.30000000000000004, 'k2': 45}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:24,581] Trial 297 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:27,437] Trial 298 finished with value: 0.8548094373865699 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9299999999999999, 'min_similarity2': 0.31, 'k2': 43}. Best is trial 209 with value: 0.8548094373865699.\n",
            "[I 2024-11-26 15:13:30,302] Trial 299 finished with value: 0.852087114337568 and parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.9199999999999999, 'min_similarity2': 0.31, 'k2': 44}. Best is trial 209 with value: 0.8548094373865699.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'threshold': -0.09999999999999998, 'k': 1, 'min_similarity1': 0.94, 'min_similarity2': 0.31, 'k2': 44}\n",
            "Best Accuracy: 0.8548094373865699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(predict, rating_matrix, sim_df, valid_df_t, threshold=study_v1.best_params['threshold'])\n",
        "\n",
        "predict_and_evaluate(predict_v2_1, rating_matrix, sim_df, valid_df_t, threshold=study_v2.best_params['threshold'], k1=study_v2.best_params['k'], k2 = study_v2.best_params['k2'], min_similarity1 = study_v2.best_params['min_similarity1'], min_similarity2 = study_v2.best_params['min_similarity2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIXfX3y4W2z8",
        "outputId": "561a5a61-7d7f-4f77-8aff-5b71456a86e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  499  161\n",
            "P  178  264\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  587   73\n",
            "P   87  355\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8548094373865699,\n",
              " 0.8031674208144797,\n",
              " 0.8294392523364486,\n",
              " 0.8160919540229885)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Grid Search 최적화"
      ],
      "metadata": {
        "id": "nSCAHC-TCV81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 후보들, min_similarity2 구하는 중\n",
        "\n",
        "min_1 = [0.9, 0.91, 0.92, 0.93, 0.94, 0.95]\n",
        "min_2 = [0.45,0.5,0.55,0.6,0.65,0.7]\n",
        "k2_values = [25,30,35,40]\n",
        "thresholds = [-0.1,0,0.1]\n",
        "# 최적의 Accuracy와 하이퍼파라미터를 저장할 변수 초기화\n",
        "best_f1 = 0\n",
        "best_acc = 0\n",
        "best_k2 = None\n",
        "best_threshold = None\n",
        "\n",
        "for threshold in thresholds:\n",
        "  for k2 in k2_values:\n",
        "      for min_similarity2 in min_2:\n",
        "        for min_similarity1 in min_1:\n",
        "          # 성능 평가\n",
        "          acc, rcs, pcs, f1 = predict_and_evaluate(predict_v2_1, rating_matrix,\n",
        "                                    sim_df, valid_df_t, threshold= threshold\n",
        "                                    , k1=1, min_similarity1 = min_similarity1, min_similarity2 = min_similarity2, k2 = k2)\n",
        "\n",
        "          # 현재 accuracy가 최고값보다 높으면 업데이트\n",
        "          if acc > best_acc:\n",
        "              best_f1 = f1\n",
        "              best_acc = acc\n",
        "              best_k2 = k2\n",
        "              best_min1 = min_similarity1\n",
        "              best_min2 = min_similarity2\n",
        "              best_threshold = threshold\n",
        "\n",
        "\n",
        "# 최적의 k2, min_similarity2, min_similarity1,threshold 출력\n",
        "print(f\"Best F1-Score: {best_f1}\")\n",
        "print(f\"Best Accuracy: {best_acc}\")\n",
        "print(f\"Best k2: {best_k2}\")\n",
        "print(f\"Best min_similarity2: {best_min2}\")\n",
        "print(f\"Best threshold: {best_threshold}\")\n",
        "print(f\"Best min_similarity1: {best_min1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wPKOfxPPCYmc",
        "outputId": "28f0a1d2-c412-4b54-8b0d-08945e20c620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8412\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8412\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8412\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8412\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8421\n",
            "Accuracy: 0.8430\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8457\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8557\n",
            "Accuracy: 0.8575\n",
            "Accuracy: 0.8593\n",
            "Accuracy: 0.8584\n",
            "Accuracy: 0.8566\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8557\n",
            "Accuracy: 0.8575\n",
            "Accuracy: 0.8593\n",
            "Accuracy: 0.8584\n",
            "Accuracy: 0.8566\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8557\n",
            "Accuracy: 0.8575\n",
            "Accuracy: 0.8584\n",
            "Accuracy: 0.8575\n",
            "Accuracy: 0.8557\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8557\n",
            "Accuracy: 0.8575\n",
            "Accuracy: 0.8584\n",
            "Accuracy: 0.8575\n",
            "Accuracy: 0.8557\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8439\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8466\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8448\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8494\n",
            "Accuracy: 0.8485\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8475\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8539\n",
            "Accuracy: 0.8530\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8503\n",
            "Accuracy: 0.8512\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8521\n",
            "Accuracy: 0.8512\n",
            "Best F1-Score: 0.8060075093867334\n",
            "Best Accuracy: 0.8593466424682396\n",
            "Best k2: 25\n",
            "Best min_similarity2: 0.45\n",
            "Best threshold: 0.1\n",
            "Best min_similarity1: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Valid 설문 결과 사용하지 않은 최적 모델 성능**"
      ],
      "metadata": {
        "id": "So_xipSz-JZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(predict_v2_1, rating_matrix, sim_df, valid_df_t,threshold= 0.1, k1 = 1, k2 = 25, min_similarity1=0.93,  min_similarity2 = 0.45)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMYe_lRe67vU",
        "outputId": "4d82dc9c-685f-41d8-fe89-8e17167e6e93"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  625   35\n",
            "P  120  322\n",
            "Accuracy: 0.8593\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8593466424682396,\n",
              " 0.7285067873303167,\n",
              " 0.9019607843137255,\n",
              " 0.8060075093867334)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Valid 설문 결과 사용하여 예측**"
      ],
      "metadata": {
        "id": "JOWZbwe5uesx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 피처 벡터 & 유사도 생성"
      ],
      "metadata": {
        "id": "MiR_rkfLF6up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 응답 결과 피처 벡터 생성"
      ],
      "metadata": {
        "id": "1xfiI-PLGkMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train, valid 데이터셋 모두 사용\n",
        "df_concat = pd.concat([df_train_t2,df_valid_t2])"
      ],
      "metadata": {
        "id": "P3yIeIIX0jVH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped2 = df_concat.groupby('img_id').agg(lambda x: calculate_mode(x) if x.name != 'img_id' else x.iloc[0])"
      ],
      "metadata": {
        "id": "CKnzsBrH0kCy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2,Q3, Q411, Q412, Q413, Q414 원핫 인코딩 수행\n",
        "grouped_one = pd.get_dummies(grouped2, columns=['Q2', 'Q3','Q411','Q412','Q413','Q414'],dtype=int)\n",
        "\n",
        "#이진 변수들 0,1 이진 변수화 진행\n",
        "fin_group = grouped_one[['Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209', 'Q4210',\n",
        "                   'Q4211','Q4212','Q4213','Q4214', 'Q4215', 'Q4216', 'Q2_1', 'Q2_2', 'Q2_3', 'Q3_1',\n",
        "                   'Q3_2', 'Q3_3', 'Q3_4', 'Q3_5', 'Q3_6', 'Q3_7', 'Q3_8', 'Q411_1', 'Q411_2', 'Q411_3','Q412_1', 'Q412_2', 'Q413_1',\n",
        "       'Q413_2', 'Q414_1', 'Q414_2']].applymap(lambda x: 1 if x != 0 else 0)\n",
        "fin_group.index = fin_group.index.astype(str)"
      ],
      "metadata": {
        "id": "Tdgne-9fRt79"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin_group_new = fin_group.copy()\n",
        "\n",
        "#fin_group 이미지 단어 변수 그룹화. 비슷한 의미를 가지는 단어들끼리 그룹화\n",
        "fin_group_new['new_1'] =(fin_group_new['Q4202'] + fin_group_new['Q4204'] + fin_group_new['Q4215'])\n",
        "fin_group_new['new_2'] = (fin_group_new['Q4205'] + fin_group_new['Q4208'] + fin_group_new['Q4216'])\n",
        "fin_group_new['new_3'] = (fin_group_new['Q4206'] + fin_group_new['Q4207'] + fin_group_new['Q4209'])\n",
        "fin_group_new['new_4'] = (fin_group_new['Q4210'] + fin_group_new['Q4211'] + fin_group_new['Q4212'] )\n",
        "fin_group_new['new_5'] = (fin_group_new['Q4213'] + fin_group_new['Q4214'])\n",
        "\n",
        "#그룹화 된 변수들은 drop\n",
        "feature_matrix_new = fin_group_new.drop(columns= ['Q4202', 'Q4203', 'Q4204', 'Q4205', 'Q4206', 'Q4207', 'Q4208', 'Q4209', 'Q4210','Q4211', 'Q4212', 'Q4213', 'Q4214', 'Q4215', 'Q4216'])"
      ],
      "metadata": {
        "id": "myAg35QbR59p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature matrix 불러와서 사용할 때\n",
        "#feature_matrix_new = pd.read_csv(\"/content/drive/MyDrive/DCC/행렬/fin_survey_feature.csv\", index_col = ['Unnamed: 0'])\n",
        "#feature_matrix_new.index = feature_matrix_new.index.astype(str).str.zfill(5)"
      ],
      "metadata": {
        "id": "CPm-mBRdGn4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 응답 결과 벡터와 이미지 피처 벡터 결합"
      ],
      "metadata": {
        "id": "FP0q6LPIHSfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터별 가중치 조절. 이미지 전체의 가중치 합 = 설문 조사 전체 가중치 합\n",
        "\n",
        "#이미지 피처 벡터\n",
        "ori_pre = ft_mat.copy()\n",
        "ori_pre = ori_pre *(1/512)\n",
        "\n",
        "#설문 결과 피처 벡터\n",
        "new_pre = ft_mat.copy()\n",
        "new_pre = feature_matrix_new *(1/25)"
      ],
      "metadata": {
        "id": "UMaZMbWYHVy4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이어붙인 feature matrix\n",
        "ft_matrix_q = pd.concat([ori_pre,new_pre],axis=1)"
      ],
      "metadata": {
        "id": "L_jgIEAjHnfK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_matrix_q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Sjl-Nq-YHpw9",
        "outputId": "6685bbf2-323a-40c9-cc98-5a237a1edb45",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "02498  0.004302  0.002615  0.000573  0.001561  0.000619  0.003800  0.003354   \n",
              "38421  0.001882  0.001731  0.003102  0.001573  0.003531  0.003027  0.002773   \n",
              "30434  0.002908  0.002350  0.001500  0.001100  0.002250  0.002782  0.000613   \n",
              "48628  0.006144  0.005431  0.001632  0.001822  0.000827  0.004789  0.005125   \n",
              "22057  0.001103  0.003216  0.000251  0.001613  0.000345  0.001056  0.002117   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "16732  0.002409  0.001336  0.000483  0.003351  0.001867  0.002403  0.000842   \n",
              "34952  0.003333  0.002878  0.000303  0.003313  0.000157  0.001925  0.002301   \n",
              "41341  0.000340  0.000864  0.000599  0.002048  0.002045  0.002834  0.001091   \n",
              "20593  0.001508  0.001709  0.002727  0.003315  0.003652  0.003049  0.002182   \n",
              "47967  0.003629  0.004367  0.001119  0.005226  0.000381  0.001833  0.003117   \n",
              "\n",
              "              7         8         9  ...  Q412_2  Q413_1  Q413_2  Q414_1  \\\n",
              "02498  0.001978  0.001528  0.000266  ...    0.00    0.04    0.00    0.00   \n",
              "38421  0.003218  0.000370  0.000260  ...    0.04    0.00    0.04    0.00   \n",
              "30434  0.002049  0.002047  0.001050  ...    0.04    0.00    0.04    0.00   \n",
              "48628  0.001201  0.000314  0.000016  ...    0.00    0.04    0.00    0.04   \n",
              "22057  0.000789  0.002091  0.000437  ...    0.00    0.04    0.00    0.04   \n",
              "...         ...       ...       ...  ...     ...     ...     ...     ...   \n",
              "16732  0.000386  0.000868  0.001149  ...    0.04    0.00    0.04    0.00   \n",
              "34952  0.001878  0.001304  0.000724  ...    0.00    0.04    0.00    0.00   \n",
              "41341  0.004434  0.001055  0.000363  ...    0.04    0.00    0.04    0.00   \n",
              "20593  0.003119  0.000659  0.001393  ...    0.00    0.04    0.00    0.04   \n",
              "47967  0.003172  0.000849  0.000334  ...    0.04    0.04    0.00    0.00   \n",
              "\n",
              "       Q414_2  new_1  new_2  new_3  new_4  new_5  \n",
              "02498    0.04   0.08   0.00   0.00   0.04   0.00  \n",
              "38421    0.04   0.00   0.00   0.00   0.00   0.04  \n",
              "30434    0.04   0.08   0.00   0.00   0.04   0.00  \n",
              "48628    0.00   0.08   0.04   0.04   0.00   0.00  \n",
              "22057    0.00   0.08   0.00   0.00   0.00   0.00  \n",
              "...       ...    ...    ...    ...    ...    ...  \n",
              "16732    0.04   0.00   0.00   0.00   0.00   0.00  \n",
              "34952    0.04   0.04   0.00   0.00   0.04   0.04  \n",
              "41341    0.04   0.08   0.04   0.00   0.04   0.00  \n",
              "20593    0.00   0.00   0.00   0.08   0.00   0.00  \n",
              "47967    0.04   0.00   0.00   0.00   0.04   0.04  \n",
              "\n",
              "[4486 rows x 537 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42a0b5c4-fd3e-48ed-bc1b-0c24bea0188b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>Q412_2</th>\n",
              "      <th>Q413_1</th>\n",
              "      <th>Q413_2</th>\n",
              "      <th>Q414_1</th>\n",
              "      <th>Q414_2</th>\n",
              "      <th>new_1</th>\n",
              "      <th>new_2</th>\n",
              "      <th>new_3</th>\n",
              "      <th>new_4</th>\n",
              "      <th>new_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>02498</th>\n",
              "      <td>0.004302</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.001561</td>\n",
              "      <td>0.000619</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.003354</td>\n",
              "      <td>0.001978</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38421</th>\n",
              "      <td>0.001882</td>\n",
              "      <td>0.001731</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>0.003531</td>\n",
              "      <td>0.003027</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>0.003218</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30434</th>\n",
              "      <td>0.002908</td>\n",
              "      <td>0.002350</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.002250</td>\n",
              "      <td>0.002782</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.002049</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48628</th>\n",
              "      <td>0.006144</td>\n",
              "      <td>0.005431</td>\n",
              "      <td>0.001632</td>\n",
              "      <td>0.001822</td>\n",
              "      <td>0.000827</td>\n",
              "      <td>0.004789</td>\n",
              "      <td>0.005125</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22057</th>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.003216</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.001056</td>\n",
              "      <td>0.002117</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.002091</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16732</th>\n",
              "      <td>0.002409</td>\n",
              "      <td>0.001336</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.003351</td>\n",
              "      <td>0.001867</td>\n",
              "      <td>0.002403</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34952</th>\n",
              "      <td>0.003333</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.001925</td>\n",
              "      <td>0.002301</td>\n",
              "      <td>0.001878</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41341</th>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.002048</td>\n",
              "      <td>0.002045</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.004434</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20593</th>\n",
              "      <td>0.001508</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>0.002727</td>\n",
              "      <td>0.003315</td>\n",
              "      <td>0.003652</td>\n",
              "      <td>0.003049</td>\n",
              "      <td>0.002182</td>\n",
              "      <td>0.003119</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.001393</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47967</th>\n",
              "      <td>0.003629</td>\n",
              "      <td>0.004367</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>0.005226</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.003117</td>\n",
              "      <td>0.003172</td>\n",
              "      <td>0.000849</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4486 rows × 537 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42a0b5c4-fd3e-48ed-bc1b-0c24bea0188b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-42a0b5c4-fd3e-48ed-bc1b-0c24bea0188b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-42a0b5c4-fd3e-48ed-bc1b-0c24bea0188b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-858896b0-d232-4d2e-8c00-7564b9561ce3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-858896b0-d232-4d2e-8c00-7564b9561ce3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-858896b0-d232-4d2e-8c00-7564b9561ce3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_17359e3e-7262-4e7a-9d8c-b25c35d54e3d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ft_matrix_q')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_17359e3e-7262-4e7a-9d8c-b25c35d54e3d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ft_matrix_q');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ft_matrix_q"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이어붙인 feature matrix의 유사도 행렬 계산\n",
        "sim_total = pd.DataFrame(cosine_similarity(ft_matrix_q), index=ft_matrix_q.index, columns=ft_matrix_q.index)\n",
        "col = sim_total.columns.astype(str).str.zfill(5)\n",
        "sim_total.columns = col\n",
        "sim_total.index = col"
      ],
      "metadata": {
        "id": "a3Le52skHvfD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 성능 확인"
      ],
      "metadata": {
        "id": "jGhUORfZIh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_evaluate(func, rating_matrix, sim_df, valid_df_t, threshold=0, k1=1, min_similarity1=0.5, k2=5, min_similarity2=0.5, min_similarity3=0.5, min_similarity4=0.5):\n",
        "  '''\n",
        "  아이템 기반 협업 필터링 결과를 확인하는 함수.\n",
        "  평가 지표 : 혼동 행렬, 정확도, f1-score\n",
        "\n",
        "  func : 아이템 협업 필터링을 위한 베이스 함수\n",
        "  rating_matrix : 유틸리티 행렬\n",
        "  sim_df : feature 유사도 행렬\n",
        "  valid_df_t : 검증 데이터\n",
        "  threshold : 선호도 판단 기준\n",
        "  k : 유사도 선택 개수\n",
        "  '''\n",
        "\n",
        "  # 모든 예측 수행\n",
        "  if func.__name__ == 'predict':\n",
        "    pred_list = [func(valid_df_t.iloc[i, 0], str(valid_df_t.iloc[i, 2]), threshold = threshold) for i in range(len(valid_df_t))]\n",
        "\n",
        "  elif func.__name__ == 'predict_v2_1':#2번 변형\n",
        "    pred_list = [func(valid_df_t.iloc[i, 0], str(valid_df_t.iloc[i, 2]), threshold = threshold, k1 = k1, min_similarity1 = min_similarity1, min_similarity2 = min_similarity2, k2 = k2) for i in range(len(valid_df_t))]\n",
        "\n",
        "  elif func.__name__ == 'predict_v3':#3번\n",
        "    pred_list = [func(valid_df_t.iloc[i, 0], str(valid_df_t.iloc[i, 2]), threshold = threshold, k1 = k1, k2 = k2, min_similarity1 = min_similarity1, min_similarity2 = min_similarity2, min_similarity3 = min_similarity3, min_similarity4 = min_similarity4) for i in range(len(valid_df_t))]\n",
        "\n",
        "  # 예측 결과 저장\n",
        "  valid_df_t['predict'] = pred_list\n",
        "\n",
        "  # 평가 지표 계산\n",
        "  conf_matrix = confusion_matrix(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  acc = accuracy_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  rcs = recall_score(valid_df_t['rating'], valid_df_t['predict'],average='binary')\n",
        "  pcs = precision_score(valid_df_t['rating'], valid_df_t['predict'])\n",
        "  f1 = f1_score(valid_df_t['rating'], valid_df_t['predict'], average='binary')\n",
        "\n",
        "  # # 결과 출력 - 하이퍼 파라미터 튜닝 중에는 잠시 출력 정지\n",
        "  print(\"Confusion Matrix:\\n\", pd.DataFrame(conf_matrix, index=['N', 'P'], columns=['N', 'P']))\n",
        "  print(f\"Accuracy: {acc:.4f}\")\n",
        "  # print(f\"Recall: {rcs:.4f}\")\n",
        "  # print(f\"Precision: {pcs:.4f}\")\n",
        "  # print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "  return acc, rcs, pcs, f1"
      ],
      "metadata": {
        "id": "ANxkBH6pIlrf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1번 Base 함수 (이미지 + 설문 Feature vector 사용)"
      ],
      "metadata": {
        "id": "0eNAvqZyIkIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(user_id, item_id, threshold):\n",
        "  '''\n",
        "  feature 유사도 행렬을 이용해서 아이템 기반 협업 필터링을 수행하는 기본적인 함수.\n",
        "\n",
        "  user_id : 사용자 식별 ID\n",
        "  item_id : 아이템 식별 ID\n",
        "  threshold : 선호도 판단 기준\n",
        "\n",
        "  rating_matrix : 유틸리티 행렬\n",
        "  sim_df : feature 유사도 행렬\n",
        "  '''\n",
        "  # 유저가 평가한 아이템의 선호도 1 또는 0 또는 Nan\n",
        "  user_ratings = rating_matrix.loc[user_id].dropna()\n",
        "\n",
        "  # 해당 아이템과 평가된 아이템 간 유사도\n",
        "  similarities = sim_total[item_id].loc[user_ratings.index]\n",
        "\n",
        "  if item_id in similarities.index: similarities.drop(index=item_id)\n",
        "\n",
        "  valid_indices = similarities.index\n",
        "\n",
        "  # 유사도와 평점의 내적 계산\n",
        "  prediction = np.dot(similarities, user_ratings) / similarities.sum()\n",
        "\n",
        "  # 임계값 기준으로 이진 분류 (1 또는 0)\n",
        "  return 1 if prediction >= threshold else -1\n",
        "\n",
        "predict_and_evaluate(predict, rating_matrix, sim_total, valid_df_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U659FLzPIpzG",
        "outputId": "278a0c9b-77a1-4e12-ecd3-5ebd077065e8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  569   91\n",
            "P  239  203\n",
            "Accuracy: 0.7005\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7005444646098004,\n",
              " 0.4592760180995475,\n",
              " 0.6904761904761905,\n",
              " 0.5516304347826086)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2번 함수 (이미지 + 설문 Feature vector 사용)"
      ],
      "metadata": {
        "id": "Uh9qCAt6JVAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_v2_1(user_id, item_id, threshold, k1, min_similarity1, min_similarity2, k2):\n",
        "      '''\n",
        "      위에 버전에서 min_similarity = 0.92에 대해 k1 = 1 인 아이템 찾음.\n",
        "      나머지 아이템 들에 대해서는 min_similarity2, k2 = k2를 적용함.\n",
        "       이게 현재 best\n",
        "\n",
        "      user_id : 사용자 식별 ID\n",
        "      item_id : 아이템 식별 ID\n",
        "      threshold : 선호도 판단 기준\n",
        "      k1: 높은 유사도 선택 개수\n",
        "      k2 : 유사도 선택 개수\n",
        "      min_similarity1 : 최소 유사도 값\n",
        "      min_similarity2: min_similarity를 만족시키지 못한 아이템들의 차선 similarity\n",
        "      rating_matrix : 유틸리티 행렬\n",
        "      sim_df : feature 유사도 행렬\n",
        "          '''\n",
        "      user_ratings = rating_matrix.loc[user_id].dropna()\n",
        "      similarities = sim_total[item_id].loc[user_ratings.index]\n",
        "\n",
        "      valid_indices_pri = similarities[similarities >= min_similarity1].nlargest(k1).index\n",
        "      valid_indices = similarities[similarities >= min_similarity2].nlargest(k2).index\n",
        "\n",
        "      #유사도가 0.92 이상인게 있으면,k1 = 1 사용\n",
        "      if len(valid_indices_pri) > 0:\n",
        "          # 유사도 0.92 이상을 만족하는 상위 1개 아이템의 유사도와 평점을 사용, 정확도 매우 좋음\n",
        "          filtered_similarities = similarities[valid_indices_pri]\n",
        "          filtered_ratings = user_ratings[valid_indices_pri]\n",
        "\n",
        "\n",
        "      #유사도가 0.92이상인게 없을 때,min_similarity2 기준으로 상위 k개 선택\n",
        "      elif len(valid_indices)>0:\n",
        "\n",
        "          filtered_similarities = similarities[valid_indices]\n",
        "          filtered_ratings = user_ratings[valid_indices]\n",
        "          weighted_sum = np.dot(filtered_similarities, filtered_ratings)\n",
        "          prediction = weighted_sum / filtered_similarities.sum()\n",
        "\n",
        "      else:\n",
        "          # 유사도가 충분하지 않아 예측 불가\n",
        "          filtered_ratings = user_ratings.values\n",
        "          filtered_similarities = similarities\n",
        "\n",
        "\n",
        "      weighted_sum = np.dot(filtered_similarities, filtered_ratings)\n",
        "      prediction = weighted_sum / filtered_similarities.sum()\n",
        "\n",
        "      # 임계값 기준으로 이진 분류 (-1 또는 1)\n",
        "\n",
        "      return 1 if prediction >= threshold else -1\n",
        "\n",
        "predict_and_evaluate(predict_v2_1, rating_matrix, sim_total, valid_df_t, threshold= -0.1, k1=1, min_similarity1 = 0.91, min_similarity2 = 0.47, k2 = 23)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwOqH9RzJW2c",
        "outputId": "124ab933-5b05-4281-c4ec-546545cf0702"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  594   66\n",
            "P   65  377\n",
            "Accuracy: 0.8811\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8811252268602541,\n",
              " 0.8529411764705882,\n",
              " 0.8510158013544018,\n",
              " 0.8519774011299435)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Grid Search 최적화"
      ],
      "metadata": {
        "id": "6PTOTmxMFmx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_1 = [0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98]\n",
        "threshold = [-0.1, 0, 0.1]\n",
        "# 최적의 Accuracy와 하이퍼파라미터를 저장할 변수 초기화\n",
        "best_f1 = 0\n",
        "best_acc = 0\n",
        "best_k2 = None\n",
        "best_threshold = None\n",
        "\n",
        "for threshold in threshold:\n",
        "    for min_similarity1 in min_1:\n",
        "      # 성능 평가\n",
        "      acc, rcs, pcs, f1 = predict_and_evaluate(predict_v2_1, rating_matrix,\n",
        "                                sim_total, valid_df_t, threshold= threshold\n",
        "                                , k1=1, min_similarity1 = min_similarity1, min_similarity2 = 0.6, k2 = 37)\n",
        "\n",
        "      # 현재 accuracy가 최고값보다 높으면 업데이트\n",
        "      if acc > best_acc:\n",
        "          best_f1 = f1\n",
        "          best_acc = acc\n",
        "          best_threshold = threshold\n",
        "          best_min1 = min_similarity1\n",
        "\n",
        "\n",
        "# 최적의 threshold와 min_similarity출력\n",
        "print(f\"Best F1-Score: {best_f1}\")\n",
        "print(f\"Best Accuracy: {best_acc}\")\n",
        "print(f\"Best threshold: {best_threshold}\")\n",
        "print(f\"Best min_similarity1: {best_min1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coZyBPsmFmGV",
        "outputId": "304c0a5b-c628-44f3-e777-e3bac0a1f82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8748\n",
            "Accuracy: 0.8748\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8748\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8639\n",
            "Accuracy: 0.8612\n",
            "Accuracy: 0.8603\n",
            "Accuracy: 0.8603\n",
            "Accuracy: 0.8603\n",
            "Accuracy: 0.8593\n",
            "Accuracy: 0.8603\n",
            "Accuracy: 0.8612\n",
            "Accuracy: 0.8603\n",
            "Best F1-Score: 0.8393063583815029\n",
            "Best Accuracy: 0.8738656987295825\n",
            "Best threshold: -0.1\n",
            "Best min_similarity1: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_2 = [0.5,0.55,0.6,0.65,0.7,0.75]\n",
        "k2_values = [10,20,30,40,50]\n",
        "threshold = [-0.1,0,0.1]\n",
        "# 최적의 Accuracy와 하이퍼파라미터를 저장할 변수 초기화\n",
        "best_f1 = 0\n",
        "best_acc = 0\n",
        "best_k2 = None\n",
        "best_threshold = None\n",
        "\n",
        "for threshold in threshold:\n",
        "  for k2 in k2_values:\n",
        "      for min_similarity2 in min_2:\n",
        "        # 성능 평가\n",
        "        acc, rcs, pcs, f1 = predict_and_evaluate(predict_v2_1, rating_matrix,\n",
        "                                  sim_total, valid_df_t, threshold= threshold\n",
        "                                  , k1=1, min_similarity1 = 0.9, min_similarity2 = min_similarity2, k2 = k2)\n",
        "\n",
        "        # 현재 Accuracy가 최고값보다 높으면 업데이트\n",
        "        if acc > best_acc:\n",
        "            best_f1 = f1\n",
        "            best_acc = acc\n",
        "            best_k2 = k2\n",
        "            best_min2 = min_similarity2\n",
        "            best_threshold = threshold\n",
        "\n",
        "\n",
        "# 최적의 min_similarity2, k, threshold 출력\n",
        "print(f\"Best F1-Score: {best_f1}\")\n",
        "print(f\"Best Accuracy: {best_acc}\")\n",
        "print(f\"Best k2: {best_k2}\")\n",
        "print(f\"Best min_similarity2: {best_min2}\")\n",
        "print(f\"Best threshold: {best_threshold}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-6_jLebGElL",
        "outputId": "cb638324-30bc-410c-cac0-1d8cf6c9564b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  595   65\n",
            "P   64  378\n",
            "Accuracy: 0.8829\n",
            "Recall: 0.8552\n",
            "Precision: 0.8533\n",
            "F1-Score: 0.8542\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  596   64\n",
            "P   61  381\n",
            "Accuracy: 0.8866\n",
            "Recall: 0.8620\n",
            "Precision: 0.8562\n",
            "F1-Score: 0.8591\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  595   65\n",
            "P   64  378\n",
            "Accuracy: 0.8829\n",
            "Recall: 0.8552\n",
            "Precision: 0.8533\n",
            "F1-Score: 0.8542\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   63  379\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.8575\n",
            "Precision: 0.8422\n",
            "F1-Score: 0.8498\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   63  379\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.8575\n",
            "Precision: 0.8422\n",
            "F1-Score: 0.8498\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  584   76\n",
            "P   66  376\n",
            "Accuracy: 0.8711\n",
            "Recall: 0.8507\n",
            "Precision: 0.8319\n",
            "F1-Score: 0.8412\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   63  379\n",
            "Accuracy: 0.8857\n",
            "Recall: 0.8575\n",
            "Precision: 0.8575\n",
            "F1-Score: 0.8575\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   57  385\n",
            "Accuracy: 0.8911\n",
            "Recall: 0.8710\n",
            "Precision: 0.8594\n",
            "F1-Score: 0.8652\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  596   64\n",
            "P   62  380\n",
            "Accuracy: 0.8857\n",
            "Recall: 0.8597\n",
            "Precision: 0.8559\n",
            "F1-Score: 0.8578\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   64  378\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8552\n",
            "Precision: 0.8419\n",
            "F1-Score: 0.8485\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   63  379\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.8575\n",
            "Precision: 0.8422\n",
            "F1-Score: 0.8498\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  584   76\n",
            "P   66  376\n",
            "Accuracy: 0.8711\n",
            "Recall: 0.8507\n",
            "Precision: 0.8319\n",
            "F1-Score: 0.8412\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  595   65\n",
            "P   62  380\n",
            "Accuracy: 0.8848\n",
            "Recall: 0.8597\n",
            "Precision: 0.8539\n",
            "F1-Score: 0.8568\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   57  385\n",
            "Accuracy: 0.8911\n",
            "Recall: 0.8710\n",
            "Precision: 0.8594\n",
            "F1-Score: 0.8652\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  596   64\n",
            "P   62  380\n",
            "Accuracy: 0.8857\n",
            "Recall: 0.8597\n",
            "Precision: 0.8559\n",
            "F1-Score: 0.8578\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   64  378\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8552\n",
            "Precision: 0.8419\n",
            "F1-Score: 0.8485\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   63  379\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.8575\n",
            "Precision: 0.8422\n",
            "F1-Score: 0.8498\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  584   76\n",
            "P   66  376\n",
            "Accuracy: 0.8711\n",
            "Recall: 0.8507\n",
            "Precision: 0.8319\n",
            "F1-Score: 0.8412\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  595   65\n",
            "P   62  380\n",
            "Accuracy: 0.8848\n",
            "Recall: 0.8597\n",
            "Precision: 0.8539\n",
            "F1-Score: 0.8568\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   57  385\n",
            "Accuracy: 0.8911\n",
            "Recall: 0.8710\n",
            "Precision: 0.8594\n",
            "F1-Score: 0.8652\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  596   64\n",
            "P   62  380\n",
            "Accuracy: 0.8857\n",
            "Recall: 0.8597\n",
            "Precision: 0.8559\n",
            "F1-Score: 0.8578\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   64  378\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8552\n",
            "Precision: 0.8419\n",
            "F1-Score: 0.8485\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   63  379\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.8575\n",
            "Precision: 0.8422\n",
            "F1-Score: 0.8498\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  584   76\n",
            "P   66  376\n",
            "Accuracy: 0.8711\n",
            "Recall: 0.8507\n",
            "Precision: 0.8319\n",
            "F1-Score: 0.8412\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  595   65\n",
            "P   62  380\n",
            "Accuracy: 0.8848\n",
            "Recall: 0.8597\n",
            "Precision: 0.8539\n",
            "F1-Score: 0.8568\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   57  385\n",
            "Accuracy: 0.8911\n",
            "Recall: 0.8710\n",
            "Precision: 0.8594\n",
            "F1-Score: 0.8652\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  596   64\n",
            "P   62  380\n",
            "Accuracy: 0.8857\n",
            "Recall: 0.8597\n",
            "Precision: 0.8559\n",
            "F1-Score: 0.8578\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   64  378\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8552\n",
            "Precision: 0.8419\n",
            "F1-Score: 0.8485\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   63  379\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.8575\n",
            "Precision: 0.8422\n",
            "F1-Score: 0.8498\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  584   76\n",
            "P   66  376\n",
            "Accuracy: 0.8711\n",
            "Recall: 0.8507\n",
            "Precision: 0.8319\n",
            "F1-Score: 0.8412\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   79  363\n",
            "Accuracy: 0.8820\n",
            "Recall: 0.8213\n",
            "Precision: 0.8768\n",
            "F1-Score: 0.8481\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  608   52\n",
            "P   80  362\n",
            "Accuracy: 0.8802\n",
            "Recall: 0.8190\n",
            "Precision: 0.8744\n",
            "F1-Score: 0.8458\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  603   57\n",
            "P   79  363\n",
            "Accuracy: 0.8766\n",
            "Recall: 0.8213\n",
            "Precision: 0.8643\n",
            "F1-Score: 0.8422\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  599   61\n",
            "P   79  363\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.8213\n",
            "Precision: 0.8561\n",
            "F1-Score: 0.8383\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  600   60\n",
            "P   76  366\n",
            "Accuracy: 0.8766\n",
            "Recall: 0.8281\n",
            "Precision: 0.8592\n",
            "F1-Score: 0.8433\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   75  367\n",
            "Accuracy: 0.8675\n",
            "Recall: 0.8303\n",
            "Precision: 0.8379\n",
            "F1-Score: 0.8341\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  614   46\n",
            "P   84  358\n",
            "Accuracy: 0.8820\n",
            "Recall: 0.8100\n",
            "Precision: 0.8861\n",
            "F1-Score: 0.8463\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  604   56\n",
            "P   82  360\n",
            "Accuracy: 0.8748\n",
            "Recall: 0.8145\n",
            "Precision: 0.8654\n",
            "F1-Score: 0.8392\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  602   58\n",
            "P   79  363\n",
            "Accuracy: 0.8757\n",
            "Recall: 0.8213\n",
            "Precision: 0.8622\n",
            "F1-Score: 0.8413\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  599   61\n",
            "P   79  363\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.8213\n",
            "Precision: 0.8561\n",
            "F1-Score: 0.8383\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  600   60\n",
            "P   76  366\n",
            "Accuracy: 0.8766\n",
            "Recall: 0.8281\n",
            "Precision: 0.8592\n",
            "F1-Score: 0.8433\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   75  367\n",
            "Accuracy: 0.8675\n",
            "Recall: 0.8303\n",
            "Precision: 0.8379\n",
            "F1-Score: 0.8341\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  615   45\n",
            "P   83  359\n",
            "Accuracy: 0.8838\n",
            "Recall: 0.8122\n",
            "Precision: 0.8886\n",
            "F1-Score: 0.8487\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  604   56\n",
            "P   82  360\n",
            "Accuracy: 0.8748\n",
            "Recall: 0.8145\n",
            "Precision: 0.8654\n",
            "F1-Score: 0.8392\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  602   58\n",
            "P   79  363\n",
            "Accuracy: 0.8757\n",
            "Recall: 0.8213\n",
            "Precision: 0.8622\n",
            "F1-Score: 0.8413\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  599   61\n",
            "P   79  363\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.8213\n",
            "Precision: 0.8561\n",
            "F1-Score: 0.8383\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  600   60\n",
            "P   76  366\n",
            "Accuracy: 0.8766\n",
            "Recall: 0.8281\n",
            "Precision: 0.8592\n",
            "F1-Score: 0.8433\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   75  367\n",
            "Accuracy: 0.8675\n",
            "Recall: 0.8303\n",
            "Precision: 0.8379\n",
            "F1-Score: 0.8341\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  615   45\n",
            "P   83  359\n",
            "Accuracy: 0.8838\n",
            "Recall: 0.8122\n",
            "Precision: 0.8886\n",
            "F1-Score: 0.8487\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  604   56\n",
            "P   82  360\n",
            "Accuracy: 0.8748\n",
            "Recall: 0.8145\n",
            "Precision: 0.8654\n",
            "F1-Score: 0.8392\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  602   58\n",
            "P   79  363\n",
            "Accuracy: 0.8757\n",
            "Recall: 0.8213\n",
            "Precision: 0.8622\n",
            "F1-Score: 0.8413\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  599   61\n",
            "P   79  363\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.8213\n",
            "Precision: 0.8561\n",
            "F1-Score: 0.8383\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  600   60\n",
            "P   76  366\n",
            "Accuracy: 0.8766\n",
            "Recall: 0.8281\n",
            "Precision: 0.8592\n",
            "F1-Score: 0.8433\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   75  367\n",
            "Accuracy: 0.8675\n",
            "Recall: 0.8303\n",
            "Precision: 0.8379\n",
            "F1-Score: 0.8341\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  615   45\n",
            "P   83  359\n",
            "Accuracy: 0.8838\n",
            "Recall: 0.8122\n",
            "Precision: 0.8886\n",
            "F1-Score: 0.8487\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  604   56\n",
            "P   82  360\n",
            "Accuracy: 0.8748\n",
            "Recall: 0.8145\n",
            "Precision: 0.8654\n",
            "F1-Score: 0.8392\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  602   58\n",
            "P   79  363\n",
            "Accuracy: 0.8757\n",
            "Recall: 0.8213\n",
            "Precision: 0.8622\n",
            "F1-Score: 0.8413\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  599   61\n",
            "P   79  363\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.8213\n",
            "Precision: 0.8561\n",
            "F1-Score: 0.8383\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  600   60\n",
            "P   76  366\n",
            "Accuracy: 0.8766\n",
            "Recall: 0.8281\n",
            "Precision: 0.8592\n",
            "F1-Score: 0.8433\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  589   71\n",
            "P   75  367\n",
            "Accuracy: 0.8675\n",
            "Recall: 0.8303\n",
            "Precision: 0.8379\n",
            "F1-Score: 0.8341\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  620   40\n",
            "P   95  347\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.7851\n",
            "Precision: 0.8966\n",
            "F1-Score: 0.8372\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  616   44\n",
            "P   94  348\n",
            "Accuracy: 0.8748\n",
            "Recall: 0.7873\n",
            "Precision: 0.8878\n",
            "F1-Score: 0.8345\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   97  345\n",
            "Accuracy: 0.8657\n",
            "Recall: 0.7805\n",
            "Precision: 0.8712\n",
            "F1-Score: 0.8234\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  607   53\n",
            "P   88  354\n",
            "Accuracy: 0.8721\n",
            "Recall: 0.8009\n",
            "Precision: 0.8698\n",
            "F1-Score: 0.8339\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   84  358\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8100\n",
            "Precision: 0.8753\n",
            "F1-Score: 0.8414\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   87  355\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.8032\n",
            "Precision: 0.8493\n",
            "F1-Score: 0.8256\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  628   32\n",
            "P  101  341\n",
            "Accuracy: 0.8793\n",
            "Recall: 0.7715\n",
            "Precision: 0.9142\n",
            "F1-Score: 0.8368\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  616   44\n",
            "P   96  346\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.7828\n",
            "Precision: 0.8872\n",
            "F1-Score: 0.8317\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   99  343\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.7760\n",
            "Precision: 0.8706\n",
            "F1-Score: 0.8206\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  607   53\n",
            "P   88  354\n",
            "Accuracy: 0.8721\n",
            "Recall: 0.8009\n",
            "Precision: 0.8698\n",
            "F1-Score: 0.8339\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   84  358\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8100\n",
            "Precision: 0.8753\n",
            "F1-Score: 0.8414\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   87  355\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.8032\n",
            "Precision: 0.8493\n",
            "F1-Score: 0.8256\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  629   31\n",
            "P  103  339\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.7670\n",
            "Precision: 0.9162\n",
            "F1-Score: 0.8350\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  616   44\n",
            "P   96  346\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.7828\n",
            "Precision: 0.8872\n",
            "F1-Score: 0.8317\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   99  343\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.7760\n",
            "Precision: 0.8706\n",
            "F1-Score: 0.8206\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  607   53\n",
            "P   88  354\n",
            "Accuracy: 0.8721\n",
            "Recall: 0.8009\n",
            "Precision: 0.8698\n",
            "F1-Score: 0.8339\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   84  358\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8100\n",
            "Precision: 0.8753\n",
            "F1-Score: 0.8414\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   87  355\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.8032\n",
            "Precision: 0.8493\n",
            "F1-Score: 0.8256\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  629   31\n",
            "P  103  339\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.7670\n",
            "Precision: 0.9162\n",
            "F1-Score: 0.8350\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  616   44\n",
            "P   96  346\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.7828\n",
            "Precision: 0.8872\n",
            "F1-Score: 0.8317\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   99  343\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.7760\n",
            "Precision: 0.8706\n",
            "F1-Score: 0.8206\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  607   53\n",
            "P   88  354\n",
            "Accuracy: 0.8721\n",
            "Recall: 0.8009\n",
            "Precision: 0.8698\n",
            "F1-Score: 0.8339\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   84  358\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8100\n",
            "Precision: 0.8753\n",
            "F1-Score: 0.8414\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   87  355\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.8032\n",
            "Precision: 0.8493\n",
            "F1-Score: 0.8256\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  629   31\n",
            "P  103  339\n",
            "Accuracy: 0.8784\n",
            "Recall: 0.7670\n",
            "Precision: 0.9162\n",
            "F1-Score: 0.8350\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  616   44\n",
            "P   96  346\n",
            "Accuracy: 0.8730\n",
            "Recall: 0.7828\n",
            "Precision: 0.8872\n",
            "F1-Score: 0.8317\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   99  343\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.7760\n",
            "Precision: 0.8706\n",
            "F1-Score: 0.8206\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  607   53\n",
            "P   88  354\n",
            "Accuracy: 0.8721\n",
            "Recall: 0.8009\n",
            "Precision: 0.8698\n",
            "F1-Score: 0.8339\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  609   51\n",
            "P   84  358\n",
            "Accuracy: 0.8775\n",
            "Recall: 0.8100\n",
            "Precision: 0.8753\n",
            "F1-Score: 0.8414\n",
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  597   63\n",
            "P   87  355\n",
            "Accuracy: 0.8639\n",
            "Recall: 0.8032\n",
            "Precision: 0.8493\n",
            "F1-Score: 0.8256\n",
            "Best F1-Score: 0.8710407239819005\n",
            "Best Accuracy: 0.8911070780399274\n",
            "Best k2: 20\n",
            "Best min_similarity2: 0.55\n",
            "Best threshold: -0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_2 = [0.45,0.46,0.47,0.48,0.49,0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6,0.61,0.62,0.63,0.64,0.65]\n",
        "k2_values = [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
        "thresholds = [-0.1,0,0.1]\n",
        "\n",
        "# 최적의 Accuracy와 하이퍼파라미터를 저장할 변수 초기화\n",
        "best_f1 = 0\n",
        "best_acc = 0\n",
        "best_k2 = None\n",
        "best_threshold = None\n",
        "\n",
        "for threshold in thresholds:\n",
        "  for k2 in k2_values:\n",
        "      for min_similarity2 in min_2:\n",
        "        # 성능 평가\n",
        "        acc, rcs, pcs, f1 = predict_and_evaluate(predict_v2_1, rating_matrix,\n",
        "                                  sim_total, valid_df_t, threshold= threshold\n",
        "                                  , k1=1, min_similarity1 = 0.91, min_similarity2 = min_similarity2, k2 = k2)\n",
        "\n",
        "        # 현재 accuracy가 최고값보다 높으면 업데이트\n",
        "        if acc > best_acc:\n",
        "            best_f1 = f1\n",
        "            best_acc = acc\n",
        "            best_k2 = k2\n",
        "            best_min2 = min_similarity2\n",
        "            best_threshold = threshold\n",
        "\n",
        "\n",
        "# 최적의 threshold, k, min_similarity 출력\n",
        "print(f\"Best F1-Score: {best_f1}\")\n",
        "print(f\"Best Accuracy: {best_acc}\")\n",
        "print(f\"Best k2: {best_k2}\")\n",
        "print(f\"Best min_similarity2: {best_min2}\")\n",
        "print(f\"Best threshold: {best_threshold}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W62SATJYJq0P",
        "outputId": "165f1b8c-77b7-4963-ebb8-40f1db02fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8748\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8902\n",
            "Accuracy: 0.8920\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8702\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8748\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8902\n",
            "Accuracy: 0.8920\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8702\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8748\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8730\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8902\n",
            "Accuracy: 0.8920\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8739\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8766\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8702\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8784\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8875\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8775\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8793\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8857\n",
            "Accuracy: 0.8884\n",
            "Accuracy: 0.8866\n",
            "Accuracy: 0.8893\n",
            "Accuracy: 0.8848\n",
            "Accuracy: 0.8820\n",
            "Accuracy: 0.8829\n",
            "Accuracy: 0.8811\n",
            "Accuracy: 0.8838\n",
            "Accuracy: 0.8802\n",
            "Accuracy: 0.8711\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8721\n",
            "Accuracy: 0.8757\n",
            "Best F1-Score: 0.8667413213885778\n",
            "Best Accuracy: 0.8920145190562614\n",
            "Best k2: 13\n",
            "Best min_similarity2: 0.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Valid 설문 결과 사용한 최적 모델 성능**"
      ],
      "metadata": {
        "id": "JN5KHDQEc75E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_evaluate(predict_v2_1, rating_matrix, sim_total, valid_df_t, threshold= -0.1, k1=1, min_similarity1 = 0.91, min_similarity2 = 0.55, k2 = 13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ-BsbaGc-NV",
        "outputId": "3650222d-6d04-4a89-e995-e63d8f368bb1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "      N    P\n",
            "N  596   64\n",
            "P   55  387\n",
            "Accuracy: 0.8920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8920145190562614,\n",
              " 0.8755656108597285,\n",
              " 0.8580931263858093,\n",
              " 0.8667413213885778)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}